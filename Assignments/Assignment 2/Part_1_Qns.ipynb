{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3d0c5bf",
   "metadata": {},
   "source": [
    "# Assignment: Exploring CLIP for Zero-Shot Learning and Linear Probing\n",
    "\n",
    "**Total Marks: 30**\n",
    "\n",
    "**Group Name:** __________________\n",
    "\n",
    "**Student Name (Student ID):**\n",
    "\n",
    "1. __________________ (__________________)\n",
    "2. __________________ (__________________)\n",
    "3. __________________ (__________________)\n",
    "4. __________________ (__________________)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This assignment aims to deepen your understanding of large-scale vision-language models like CLIP (Contrastive Language-Image Pre-Training). You will explore two primary methods for applying CLIP to image classification tasks:\n",
    "\n",
    "1.  **Zero-Shot Prediction**: Using CLIP's ability to associate images with arbitrary text descriptions without any task-specific training. You will implement the core prediction logic and investigate how different \"prompts\" and similarity metrics affect performance.\n",
    "2.  **Linear Probing**: Using CLIP as a feature extractor. You will freeze the powerful pre-trained image encoder and **build and train** a simple linear classifier on top of these features.\n",
    "\n",
    "You will compare the performance, trade-offs, and characteristics of these two approaches using the ImageNet dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0599ef6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Question 1: Setting up CLIP** [6 Marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0c4cf3",
   "metadata": {},
   "source": [
    "### **Step 1: Environment Setup and Model Loading (Provided)**\n",
    "\n",
    "This section loads the pre-trained CLIP model and processor. This part is provided so you can focus on the core logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad90f013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "from datasets import Features, Sequence, Value\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, List, Dict\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset as TorchDataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "CACHE_DIR = \"./cache\" # Cache directory to save CLIP features\n",
    "\n",
    "model_str = \"openai/clip-vit-base-patch32\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Load the CLIP model with sdpa attention implementation for efficiency\n",
    "model = CLIPModel.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_str, attn_implementation=\"sdpa\"\n",
    ").to(device)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Load the processor for preparing image and text data\n",
    "processor = CLIPProcessor.from_pretrained(model_str)\n",
    "\n",
    "print(f\"Model loaded on device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390502e9",
   "metadata": {},
   "source": [
    "### **Step 2: A Quick Demonstration (Provided)**\n",
    "\n",
    "This section loads a demonstration image from the web to show a simple example of what CLIP can do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3d2c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_image_url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "print(f\"Loading a demo image from: {demo_image_url}\")\n",
    "demo_image = Image.open(requests.get(demo_image_url, stream=True).raw).convert(\"RGB\")\n",
    "demo_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f6b644",
   "metadata": {},
   "source": [
    "### **Step 3: Implementing the Core CLIP Encoder [3 Marks]**\n",
    "\n",
    "**Your Task:** Implement the core functionalities of the `CLIPEncoder` class. This class is central to performing zero-shot predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6e273e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPEncoder:\n",
    "    \"\"\"\n",
    "    A class that encapsulates the CLIP model's encoding functionalities for images and text.\n",
    "    \"\"\"\n",
    "    def __init__(self, model: CLIPModel, processor: CLIPProcessor, device: torch.device):\n",
    "        self.model = model\n",
    "        self.processor = processor\n",
    "        self.device = device\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def encode_image(self, images: List[Image.Image]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encodes a batch of PIL images into feature vectors.\n",
    "\n",
    "        Instructions:\n",
    "        1. Use `self.processor` to process the list of images. Ensure tensors are returned (\"pt\").\n",
    "        2. Move the processed inputs to `self.device`.\n",
    "        3. Use `self.model.get_image_features` to get the embeddings for the batch.\n",
    "\n",
    "        Helpful Documentation ðŸ“š:\n",
    "        - CLIPProcessor: https://huggingface.co/docs/transformers/main/en/model_doc/clip#transformers.CLIPProcessor\n",
    "        - CLIPModel.get_image_features: https://huggingface.co/docs/transformers/main/en/model_doc/clip#transformers.CLIPModel.get_image_features\n",
    "        \"\"\"\n",
    "        # ================================\n",
    "        # TODO: YOUR CODE GOES HERE (1 Mark)\n",
    "        # ================================\n",
    "        raise NotImplementedError(\"Please implement the `encode_image` method for batch processing.\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode_text(self, text: List[str]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encodes a list of texts into feature vectors.\n",
    "        \n",
    "        Instructions:\n",
    "        1. Use `self.processor` to tokenize the text. Ensure tensors are returned (\"pt\"),\n",
    "           and that padding and truncation are enabled.\n",
    "        2. Move the processed inputs to `self.device`.\n",
    "        3. Use `self.model.get_text_features` to get the embeddings.\n",
    "        \n",
    "        Helpful Documentation ðŸ“š:\n",
    "        - CLIPProcessor: https://huggingface.co/docs/transformers/main/en/model_doc/clip#transformers.CLIPProcessor\n",
    "        - CLIPModel.get_text_features: https://huggingface.co/docs/transformers/main/en/model_doc/clip#transformers.CLIPModel.get_text_features\n",
    "        \"\"\"\n",
    "        # ================================\n",
    "        # TODO: YOUR CODE GOES HERE (1 Mark)\n",
    "        # ================================\n",
    "        raise NotImplementedError(\"Please implement the `encode_text` method.\")\n",
    "\n",
    "    @staticmethod\n",
    "    @torch.no_grad()\n",
    "    def predict(image_features: torch.Tensor, text_features: torch.Tensor, criterion: str) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Calculates the similarity between image and text features and returns predictions.\n",
    "\n",
    "        (... documentation omitted for brevity ...)\n",
    "        \"\"\"\n",
    "        assert criterion in [\"dot_product\", \"cosine_similarity\"], \"Invalid criterion\"\n",
    "        \n",
    "        # ================================\n",
    "        # TODO: YOUR CODE GOES HERE (1 Mark)\n",
    "        # ================================\n",
    "        raise NotImplementedError(\"Please implement the `predict` method.\")\n",
    "\n",
    "# Initialize the encoder (this will use your implementation)\n",
    "encoder = CLIPEncoder(model, processor, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49a25aa",
   "metadata": {},
   "source": [
    "### **Step 4: Test Case for the Encoder (Provided)**\n",
    "\n",
    "This section provides a simple test to check if your encoder implementation is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393e17f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple test on the demo image\n",
    "try:\n",
    "    demo_labels = [\"a photo of a cat\", \"a photo of a dog\", \"a photo of two cats\"]\n",
    "    image_features = encoder.encode_image([demo_image]) \n",
    "    text_features = encoder.encode_text(demo_labels)\n",
    "    predicted_idx, logits = encoder.predict(image_features, text_features, \"cosine_similarity\")\n",
    "    print(f\"Predicted Label: '{demo_labels[predicted_idx.item()]}'\")\n",
    "    print(f\"Probabilities: {logits.cpu().numpy().flatten()}\")\n",
    "except NotImplementedError:\n",
    "    print(\"One or more methods in CLIPEncoder are not yet implemented.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecf0f35",
   "metadata": {},
   "source": [
    "### **Step 5: Data Loading and Preparation (Provided)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e917dc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imagenet_val_clip_features(\n",
    "    encoder: \"CLIPEncoder\",\n",
    "    cache_path: str = os.path.join(CACHE_DIR, \"imagenet_val_clip\"),\n",
    "    batch_size: int = 256,\n",
    "    tqdm_total: int | None = 50000  # ImageNet-1k val has 50,000 images\n",
    ") -> datasets.Dataset:\n",
    "    \"\"\"\n",
    "    Stream the full ImageNet validation split, encode images into CLIP vectors,\n",
    "    cache features+labels to disk, and return a Dataset that yields torch tensors.\n",
    "    \"\"\"\n",
    "    if os.path.exists(cache_path):\n",
    "        print(\"Loading CLIP features from local cache...\")\n",
    "        ds = datasets.Dataset.load_from_disk(cache_path)\n",
    "        ds.set_format(type=\"torch\", columns=[\"features\", \"label\"])\n",
    "        return ds\n",
    "\n",
    "    print(\"Downloading ImageNet validation (streaming mode)...\")\n",
    "    stream_dataset = datasets.load_dataset(\n",
    "        \"benjamin-paine/imagenet-1k-256x256\",\n",
    "        split=\"validation\",\n",
    "        streaming=True\n",
    "    )\n",
    "\n",
    "    feats_chunks: list[torch.Tensor] = []\n",
    "    labels_all: list[int] = []\n",
    "    buf_images, buf_labels = [], []\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def flush():\n",
    "        nonlocal feats_chunks, labels_all, buf_images, buf_labels\n",
    "        if not buf_images:\n",
    "            return\n",
    "        \n",
    "        # Calling the refactored method.\n",
    "        feats_on_gpu = encoder.encode_image(buf_images)\n",
    "        feats = feats_on_gpu.to(\"cpu\", dtype=torch.float32)\n",
    "        \n",
    "        feats_chunks.append(feats)\n",
    "        labels_all.extend(buf_labels)\n",
    "        buf_images, buf_labels = [], []\n",
    "\n",
    "    print(\"Encoding full ImageNet validation set with CLIP...\")\n",
    "    pbar = tqdm(stream_dataset, desc=\"Encoding\", total=tqdm_total)\n",
    "    for row in pbar:\n",
    "        buf_images.append(row[\"image\"])\n",
    "        buf_labels.append(int(row[\"label\"]))\n",
    "        if len(buf_images) >= batch_size:\n",
    "            flush()\n",
    "    \n",
    "    flush() # Process any remaining images\n",
    "    pbar.close()\n",
    "\n",
    "    all_features = torch.cat(feats_chunks, dim=0).contiguous().numpy().astype(np.float32)\n",
    "    labels_np = np.asarray(labels_all, dtype=np.int64)\n",
    "\n",
    "    feature_dim = all_features.shape[1]\n",
    "    schema = Features({\n",
    "        \"features\": Sequence(Value(\"float32\"), length=feature_dim),\n",
    "        \"label\": Value(\"int64\"),\n",
    "    })\n",
    "    \n",
    "    ds = datasets.Dataset.from_dict({\"features\": all_features, \"label\": labels_np}, features=schema)\n",
    "\n",
    "    ds.save_to_disk(cache_path)\n",
    "    print(\"CLIP features cached locally!\")\n",
    "\n",
    "    ds.set_format(type=\"torch\", columns=[\"features\", \"label\"])\n",
    "    return ds\n",
    "\n",
    "# The code below will run your function and prepare the final training and test sets.\n",
    "dataset = load_imagenet_val_clip_features(\n",
    "    encoder,\n",
    "    cache_path=os.path.join(CACHE_DIR, \"imagenet_val_clip\"),\n",
    "    batch_size=256\n",
    ")\n",
    "\n",
    "with Path(\"./id_to_label.json\").open(\"r\") as f:\n",
    "    idx_to_label = json.load(f)\n",
    "\n",
    "class_labels = list(idx_to_label.values())\n",
    "\n",
    "all_features, all_labels = zip(*[(item[\"features\"], int(item[\"label\"])) for item in dataset])\n",
    "all_features = torch.stack(all_features).to(device)\n",
    "all_labels = torch.tensor(all_labels, dtype=torch.int64).to(device)\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(\n",
    "    all_features, all_labels, test_size=0.2, shuffle=True, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train set shape: {train_images.shape}\")\n",
    "print(f\"Test set shape: {test_images.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdaf852",
   "metadata": {},
   "source": [
    "### **Step 6: Implementing Evaluation Metrics [3 Marks]**\n",
    "\n",
    "**Your Task:** Implement the `accuracy_score` and `f1_score` methods. Correctly calculating metrics is fundamental to any machine learning task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfe92e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics:\n",
    "    \"\"\"\n",
    "    Provides static methods for calculating evaluation metrics.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def _ensure_tensor(x, device=None) -> torch.Tensor:\n",
    "        # This is a helper function, provided for convenience.\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x)\n",
    "        if device is not None:\n",
    "            x = x.to(device)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def accuracy_score(predictions: torch.Tensor | List, targets: torch.Tensor | List, device=None) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the classification accuracy. (2 Marks)\n",
    "        (... documentation omitted for brevity ...)\n",
    "        \"\"\"\n",
    "        # ================================\n",
    "        # TODO: YOUR CODE GOES HERE\n",
    "        # ================================\n",
    "        raise NotImplementedError(\"Please implement the `accuracy_score` method.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def f1_score(predictions: torch.Tensor | List, targets: torch.Tensor | List,\n",
    "                 num_classes: Optional[int] = None, device=None) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the macro-average F1 score. (2 Marks)\n",
    "        (... documentation omitted for brevity ...)\n",
    "        \"\"\"\n",
    "        # ================================\n",
    "        # TODO: YOUR CODE GOES HERE\n",
    "        # ================================\n",
    "        raise NotImplementedError(\"Please implement the `f1_score` method.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4f7331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block will check your metric implementations against numpy/sklearn for a sample case.\n",
    "try:\n",
    "    # Generate some dummy data\n",
    "    sample_preds = [0, 1, 2, 0, 1, 2, 0, 1, 2]\n",
    "    sample_targets = [0, 2, 1, 0, 2, 1, 0, 0, 1]\n",
    "    metrics = Metrics()\n",
    "    \n",
    "    # Test accuracy\n",
    "    accuracy_torch = metrics.accuracy_score(sample_preds, sample_targets)\n",
    "    accuracy_np = np.mean(np.array(sample_preds) == np.array(sample_targets))\n",
    "    print(f\"Accuracy (yours): {accuracy_torch:.4f}, Accuracy (numpy): {accuracy_np:.4f}\")\n",
    "    assert np.isclose(accuracy_torch, accuracy_np), \"Accuracy mismatch!\"\n",
    "\n",
    "    # Test F1 score\n",
    "    from sklearn.metrics import f1_score as f1_sklearn\n",
    "    f1_torch = metrics.f1_score(sample_preds, sample_targets)\n",
    "    f1_np = f1_sklearn(sample_targets, sample_preds, average='macro')\n",
    "    print(f\"F1 Score (yours): {f1_torch:.4f}, F1 Score (sklearn): {f1_np:.4f}\")\n",
    "    assert np.isclose(f1_torch, f1_np), \"F1 Score mismatch!\"\n",
    "\n",
    "    print(\"\\nâœ… All metrics passed the sanity check!\")\n",
    "except (NotImplementedError, AssertionError) as e:\n",
    "    print(f\"Metrics check failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4e446d",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## **Question 2: Zero-Shot Classification Analysis** [6 Marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcf7605",
   "metadata": {},
   "source": [
    "### **Step 7: The Art of Prompt Engineering** [2 Marks]\n",
    "\n",
    "**Your Task:** Investigate how different text prompts affect classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66879815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_zero_shot_prompts(\n",
    "    prompt_templates: List[str], \n",
    "    class_labels: List[str], \n",
    "    test_images: torch.Tensor, \n",
    "    test_labels: torch.Tensor, \n",
    "    encoder: \"CLIPEncoder\",\n",
    "    metrics: \"Metrics\"\n",
    ") -> Dict[str, float]:\n",
    "    results = {}\n",
    "    print(\"Evaluating different prompts...\")\n",
    "    for template in tqdm(prompt_templates, desc=\"Prompts\"):\n",
    "        # ================================\n",
    "        # TODO: YOUR CODE GOES HERE\n",
    "        # ================================\n",
    "        raise NotImplementedError(\"Please implement the prompt evaluation loop.\")\n",
    "    return results\n",
    "\n",
    "prompts_to_test = [\n",
    "    \"a photo of a {}\", \n",
    "    \"a picture of a {}\", \n",
    "    \"an image of a {}\", \n",
    "    \"{}\",\n",
    "    \"a wild animal: {}\",\n",
    "    \"Beneath the fading sunset, the curious child wandered along the winding path. This is an image of {}.\"\n",
    "]\n",
    "\n",
    "prompt_accuracies = evaluate_zero_shot_prompts(prompts_to_test, class_labels, test_images, test_labels, encoder, metrics)\n",
    "for prompt, accuracy in prompt_accuracies.items():\n",
    "    print(f'Prompt: \"{prompt}\" -> Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350551c9",
   "metadata": {},
   "source": [
    "### **Step 8: Analysis for Prompt Engineering** [1 Mark]\n",
    "**(Your analysis goes here)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ace3251",
   "metadata": {},
   "source": [
    "### **Step 9: Similarity Metric Comparison** [2 Marks]\n",
    "\n",
    "**Your Task:** Compare the performance of `dot_product` and `cosine_similarity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45722d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_prompt, accuracy_dot, accuracy_cos = None, None, None\n",
    "\"\"\"\n",
    "Instructions:\n",
    "1. Choose your best prompt from Step 7.\n",
    "2. Generate and encode the corresponding text labels.\n",
    "3. Predict using 'dot_product' and calculate accuracy.\n",
    "4. Predict using 'cosine_similarity' and calculate accuracy.\n",
    "5. Print the results clearly.\n",
    "\"\"\"\n",
    "# ================================\n",
    "# TODO: YOUR CODE GOES HERE [1 Mark]\n",
    "# ================================\n",
    "\n",
    "print(f\"Using prompt: '{best_prompt}'\\n\")\n",
    "print(f\"Accuracy with dot_product: {accuracy_dot:.4f}\")\n",
    "print(f\"Accuracy with cosine_similarity: {accuracy_cos:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1905527e",
   "metadata": {},
   "source": [
    "### **Step 10: Analysis for Similarity Metric Comparison** [1 Mark]\n",
    "**(Your analysis goes here)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc88a69d",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## **Question 3: Linear Probing** [8 Marks]\n",
    "\n",
    "Linear Probing is a techinique of training a simple classification network by freezing the backbone. I.e., during this stage, the encoder (CLIP in our case) is frozen and not updated, while only the downstream classification head is trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaa2f07",
   "metadata": {},
   "source": [
    "### **Step 11: Implementing the Classification Network** [2 Marks]\n",
    "\n",
    "**Your Task:** Implement the `ClsNetwork`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60ced54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualFCNBlock(nn.Module): # Provided\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, p: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(input_dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Dropout(p),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x + self.ff(self.norm(x))\n",
    "\n",
    "class ClsNetwork(nn.Module):\n",
    "    def __init__(self, input_dim: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        hidden_dim = 4 * input_dim\n",
    "        self.res_blk = ResidualFCNBlock(input_dim, hidden_dim, p=0.2)\n",
    "        # ================================\n",
    "        # TODO: YOUR CODE GOES HERE (1 Mark)\n",
    "        # ================================\n",
    "        raise NotImplementedError(\"Please define the layers in ClsNetwork `__init__`\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # ================================\n",
    "        # TODO: YOUR CODE GOES HERE (1 Mark)\n",
    "        # ================================\n",
    "        raise NotImplementedError(\"Please implement the `forward` method of ClsNetwork.\")\n",
    "\n",
    "classifier_model = ClsNetwork(input_dim=512, num_classes=len(class_labels)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7126d92b",
   "metadata": {},
   "source": [
    "### **Step 12: Implementing the Training Step** [2 Marks]\n",
    "\n",
    "**Your Task:** Implement the core training step inside the `Trainer.train` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd5c0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, test_loader, device):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.device = device\n",
    "        self.writer = SummaryWriter('runs/linear_probe_experiment')\n",
    "\n",
    "    def train(self, epochs: int, lr: float):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0.0\n",
    "            pbar = tqdm(self.train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False)\n",
    "            for features, labels in pbar:\n",
    "                features, labels = features.to(self.device), labels.to(self.device)\n",
    "                # ================================\n",
    "                # TODO: YOUR CODE GOES HERE (1 Mark): The core training step\n",
    "                # ================================\n",
    "                raise NotImplementedError(\"Please implement the core training step.\")\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                pbar.set_postfix(loss=f\"{total_loss / (pbar.n + 1):.4f}\")\n",
    "\n",
    "            self.writer.add_scalar('Loss/train', total_loss / len(self.train_loader), epoch)\n",
    "            self.evaluate(epoch)\n",
    "        self.writer.close()\n",
    "        print(\"\\nTraining finished!\")\n",
    "\n",
    "    def evaluate(self, epoch: int):\n",
    "        self.model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for features, labels in self.test_loader:\n",
    "                features, labels = features.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(features)\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                all_preds.extend(preds.cpu().tolist())\n",
    "                all_labels.extend(labels.cpu().tolist())\n",
    "        metrics = Metrics()\n",
    "        acc = metrics.accuracy_score(all_preds, all_labels)\n",
    "        f1  = metrics.f1_score(all_preds, all_labels)\n",
    "        print(f\"Epoch {epoch+1} Test Metrics: accuracy={acc:.4f}, f1={f1:.4f}\")\n",
    "        self.writer.add_scalar('Accuracy/test', acc, epoch)\n",
    "        self.writer.add_scalar('F1_Score/test', f1, epoch)\n",
    "\n",
    "class ClsDataset(TorchDataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.features)\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "train_dataset = ClsDataset(train_images, train_labels)\n",
    "test_dataset = ClsDataset(test_images, test_labels)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# ================================\n",
    "# TODO: YOUR CODE (1 Mark): Instantiate and run the trainer\n",
    "# ================================\n",
    "raise NotImplementedError(\"Please instantiate and call the Trainer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2b6212",
   "metadata": {},
   "source": [
    "### **Step 13: Performance Analysis and Comparison** [4 Marks]\n",
    "\n",
    "**Your Task:** After completing the code above, answer the following analysis questions. *Write your answers in the Markdown cell below AND in the PDF report.*\n",
    "\n",
    "1.  Report the **final test accuracy and F1-score** of your trained linear classifier. (1 Mark)\n",
    "2.  Create a summary table comparing the performance (Accuracy) of Zero-Shot Classification and the Trained Linear Classifier. (1 Mark)\n",
    "3.  Analyze the results. Why does the trained classifier significantly outperform the zero-shot approach? (1 Mark)\n",
    "4.  Discuss the **trade-offs** between these two methods (Performance, Data Requirement, Flexibility, Computational Cost). (1 Mark)\n",
    "\n",
    "**(Your analysis goes here)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18794c92",
   "metadata": {},
   "source": [
    "## **Question 4: Paper Reading** [10 Marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767910ff",
   "metadata": {},
   "source": [
    "### **Step 14: Questions for Paper Reading** \n",
    "\n",
    "Read the paper titled ``Learning Transferable Visual Models From Natural Language Supervision\" (https://arxiv.org/abs/2103.00020) and answer the following questions. The answers must be included in your report.\n",
    "\n",
    "1. What does the CLIP model learn? [1 mark]\n",
    "2. Explain in at most 3 sentences what \"contrastive learning\" means. [1 mark]\n",
    "3. Why do you think CLIPâ€™s zero-shot performance can sometimes surpass supervised baselines? What does this say about the generalization abilities of representation learning? [2 marks]\n",
    "4. How do the labels in CLIP-based zero-shot classification differ from traditional image classification models from supervised learning? [2 marks]\n",
    "5. What do you think CLIP falls short in, and why do you think this happens? [4 marks]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d722a33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
