{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e601616",
   "metadata": {},
   "source": [
    "# Assignment: Fine-tuning CLIP for Image–Text Retrieval (Part 2)\n",
    "\n",
    "**Name (Student ID):**  \n",
    "1. __________________ (________________)\n",
    "2. __________________ (________________)\n",
    "3. __________________ (________________)\n",
    "4. __________________ (________________)\n",
    "\n",
    "**Total Marks: 25**  \n",
    "**Rubric:** Each step shows its mark weight. Auto-check cells provide partial verification. Keep the notebook runnable on a personal computer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f9f215",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "In Part 1, CLIP was used as a fixed feature extractor. In this part, you will **fine-tune** CLIP for image–text retrieval on a small paired dataset (**Flickr8k**) using a **parameter-efficient** approach. We will **freeze** the image and text encoders and train only the **projection layers**.\n",
    "\n",
    "**Tasks**\n",
    "- Load and split Flickr8k, wrap it with a PyTorch `Dataset` and `DataLoader`.\n",
    "- Implement two contrastive objectives: **InfoNCE** and **Triplet Loss**.\n",
    "- Fine-tune only the projection layers and compare with a **zero-shot** baseline.\n",
    "- Evaluate with **Recall@K** (I2T and T2I).\n",
    "- Run small experiments and answer short questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5cdc81",
   "metadata": {},
   "source": [
    "> **Note**  \n",
    "> This notebook is designed to be runnable on a typical laptop. If memory is tight, reduce `batch_size` and/or the number of training epochs. Avoid large image sizes. Keep the encoders frozen to limit compute.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf6ac65",
   "metadata": {},
   "source": [
    "### Step 1: Setup and Imports (Provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405e3677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provided: core imports\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2a435e",
   "metadata": {},
   "source": [
    "### Step 2: Load CLIP and Enable PEFT [3 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96b5bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_str = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "# TODO: Load model and processor (similar to Part 1)\n",
    "# Hints: use CLIPModel.from_pretrained / CLIPProcessor.from_pretrained\n",
    "# Move model to device.\n",
    "model = None\n",
    "processor = None\n",
    "\n",
    "# TODO: Freeze text and vision encoders; unfreeze only projection layers\n",
    "def setup_peft_model(model: \"CLIPModel\") -> \"CLIPModel\":\n",
    "    \"\"\"Freeze encoders, train only projection layers.\"\"\"\n",
    "    # 1) Freeze encoders\n",
    "    # 2) Unfreeze model.text_projection and model.visual_projection\n",
    "    # 3) Return model\n",
    "    raise NotImplementedError\n",
    "\n",
    "# After loading model and processor above, call:\n",
    "# model = setup_peft_model(model)\n",
    "\n",
    "# Diagnostics (will be used by auto-checks below)\n",
    "def count_params(model):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable, 100.0 * trainable / max(1, total)\n",
    "\n",
    "# Uncomment after you implement:\n",
    "# total, trainable, pct = count_params(model)\n",
    "# print(f\"Total parameters: {total:,}\")\n",
    "# print(f\"Trainable parameters: {trainable:,} ({pct:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc8f7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Auto-check: Step 2 ===\n",
    "try:\n",
    "    assert model is not None and processor is not None, \"Model/processor not initialized.\"\n",
    "    # encoders must be frozen\n",
    "    enc_frozen = all(not p.requires_grad for p in model.text_model.parameters()) and all(not p.requires_grad for p in model.vision_model.parameters())\n",
    "    assert enc_frozen, \"Encoders should be frozen.\"\n",
    "    # projections must be trainable\n",
    "    assert any(p.requires_grad for p in model.text_projection.parameters()), \"text_projection should be trainable.\"\n",
    "    assert any(p.requires_grad for p in model.visual_projection.parameters()), \"visual_projection should be trainable.\"\n",
    "    print(\"Step 2 check passed.\")\n",
    "except Exception as e:\n",
    "    print(\"Step 2 check failed:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bc885f",
   "metadata": {},
   "source": [
    "### Step 3: Data Preparation (Flickr8k) [4 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c761826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the Flickr8k dataset and split into train/test indices\n",
    "# Use the split sizes: 6000 for training, remaining for test (~1000).\n",
    "# Keep a fixed random seed for reproducibility.\n",
    "# Hints: flickr = load_dataset(\"Naveengo/flickr8k\") ; all_data = flickr[\"train\"]\n",
    "\n",
    "flickr = None\n",
    "train_indices, test_indices = None, None\n",
    "\n",
    "# TODO: Implement a custom Dataset that packs pixel_values, input_ids, attention_mask\n",
    "class FlickrDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, indices, processor):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.indices = indices\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Map local index to original index\n",
    "        raise NotImplementedError\n",
    "\n",
    "# TODO: Create train_dataset/test_dataset and DataLoaders (batch_size <= 32)\n",
    "train_dataset = None\n",
    "test_dataset = None\n",
    "train_loader = None\n",
    "test_loader = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a69163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Auto-check: Step 3 ===\n",
    "try:\n",
    "    assert train_dataset is not None and test_dataset is not None, \"Datasets not built.\"\n",
    "    assert train_loader is not None and test_loader is not None, \"DataLoaders not built.\"\n",
    "    # quick sample\n",
    "    sample = next(iter(train_loader))\n",
    "    for k in (\"pixel_values\",\"input_ids\",\"attention_mask\"):\n",
    "        assert k in sample, f\"Missing key in batch: {k}\"\n",
    "    bs = sample[\"pixel_values\"].shape[0]\n",
    "    assert bs >= 1, \"Empty batch.\"\n",
    "    print(\"Step 3 check passed.\")\n",
    "except Exception as e:\n",
    "    print(\"Step 3 check failed:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdc0174",
   "metadata": {},
   "source": [
    "### Step 4: Contrastive Losses and Recall@K [7 marks: InfoNCE 3m, Triplet 3m, Recall@K: 1m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc87c9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement InfoNCE loss\n",
    "def info_nce_loss(image_features: torch.Tensor, text_features: torch.Tensor, temperature: float = 0.07):\n",
    "    \"\"\"Return scalar loss.\"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "# TODO: Implement simple triplet loss with cosine distance and a rolled negative\n",
    "def triplet_loss(image_features: torch.Tensor, text_features: torch.Tensor, margin: float = 0.2):\n",
    "    raise NotImplementedError\n",
    "\n",
    "# TODO: Implement Recall@K for I2T and T2I in a light-weight way (no gradients)\n",
    "@torch.no_grad()\n",
    "def calculate_recall_at_k(model, test_dataset, device, k_values=(1,5,10)):\n",
    "    \"\"\"Return a dict with keys: I2T_R@1, I2T_R@5, I2T_R@10, T2I_R@1, T2I_R@5, T2I_R@10.\"\"\"\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de37a3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Auto-check: Step 4 ===\n",
    "try:\n",
    "    # create random embeddings to sanity-check loss shapes\n",
    "    a = torch.randn(8, 512)\n",
    "    b = torch.randn(8, 512)\n",
    "    l1 = info_nce_loss(a, b)\n",
    "    l2 = triplet_loss(a, b)\n",
    "    assert torch.is_tensor(l1) and l1.ndim == 0, \"InfoNCE must return a scalar tensor.\"\n",
    "    assert torch.is_tensor(l2) and l2.ndim == 0, \"Triplet loss must return a scalar tensor.\"\n",
    "    print(\"Loss functions basic shape check passed.\")\n",
    "except Exception as e:\n",
    "    print(\"Step 4 loss checks failed:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1c5c9a",
   "metadata": {},
   "source": [
    "### Step 5: Trainer [4 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdc8fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement a small trainer that accepts a loss function\n",
    "class RetrievalTrainer:\n",
    "    def __init__(self, model, train_loader, test_dataset, device):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.test_dataset = test_dataset\n",
    "        self.device = device\n",
    "        self.writer = None\n",
    "\n",
    "    def train(self, epochs: int, lr: float, loss_fn, loss_name: str):\n",
    "        # Hints: Use AdamW over trainable params only; normalize embeddings; log scalar loss each epoch\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def evaluate(self, epoch: int):\n",
    "        # Hints: call calculate_recall_at_k and print metrics; log to TensorBoard as well\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a6ee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Auto-check: Step 5 (structure only) ===\n",
    "try:\n",
    "    t = RetrievalTrainer\n",
    "    assert callable(getattr(t, \"train\")), \"train() missing.\"\n",
    "    assert callable(getattr(t, \"evaluate\")), \"evaluate() missing.\"\n",
    "    print(\"Step 5 structure check passed.\")\n",
    "except Exception as e:\n",
    "    print(\"Step 5 check failed:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f91379",
   "metadata": {},
   "source": [
    "### Step 6: Experiments [Partial Provided, 2 marks for completion]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b1caa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline: zero-shot metrics without fine-tuning\n",
    "# Note: This will be slower on CPU. Consider reducing test set size temporarily for a quick smoke test.\n",
    "print(\"--- Calculating Zero-Shot Baseline Performance ---\")\n",
    "try:\n",
    "    baseline_metrics = calculate_recall_at_k(model, test_dataset, device)\n",
    "    for k, v in baseline_metrics.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "except Exception as e:\n",
    "    print(\"Baseline evaluation failed (ok if you have not implemented calculate_recall_at_k yet):\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f15468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1: InfoNCE fine-tuning\n",
    "# Reset model to fresh PEFT state\n",
    "try:\n",
    "    model = CLIPModel.from_pretrained(model_str).to(device)\n",
    "    model = setup_peft_model(model)\n",
    "    trainer = RetrievalTrainer(model, train_loader, test_dataset, device)\n",
    "    learning_rate = 1e-5\n",
    "    num_epochs = 3  # keep small for resource limits\n",
    "    trainer.train(epochs=num_epochs, lr=learning_rate, loss_fn=info_nce_loss, loss_name=\"InfoNCE\")\n",
    "except Exception as e:\n",
    "    print(\"InfoNCE experiment not executed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422c74c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: Triplet fine-tuning\n",
    "try:\n",
    "    model = CLIPModel.from_pretrained(model_str).to(device)\n",
    "    model = setup_peft_model(model)\n",
    "    trainer = RetrievalTrainer(model, train_loader, test_dataset, device)\n",
    "    learning_rate = 1e-5\n",
    "    num_epochs = 3\n",
    "    trainer.train(epochs=num_epochs, lr=learning_rate, loss_fn=triplet_loss, loss_name=\"Triplet\")\n",
    "except Exception as e:\n",
    "    print(\"Triplet experiment not executed:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87f0704",
   "metadata": {},
   "source": [
    "### Step 7: Analysis[5 marks]\n",
    "\n",
    "**Your Task:** After completing the code above, answer the following analysis questions. *Write your answers in the Markdown cell below AND in the PDF report.*\n",
    "\n",
    "1. **Performance Comparison:** Compare your fine-tuned results (InfoNCE vs Triplet) with the zero-shot baseline. Which improved Recall@K more on Flickr8k and why?  \n",
    "2. **Convergence:** Show training loss plots (TensorBoard) and discuss which objective converged more smoothly.  \n",
    "3. **Learning Rate Sweep:** Try `1e-4` and `1e-6` for InfoNCE. Summarize changes in Recall@K and loss curves.  \n",
    "4. **Overfitting:** Train longer with the better setup and discuss if/when overfitting arises.  \n",
    "5. **Qualitative:** One success and one failure case. Speculate on causes.\n",
    "\n",
    "**(Your analysis goes here)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca006bdd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
