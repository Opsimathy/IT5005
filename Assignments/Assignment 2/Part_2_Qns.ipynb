{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e601616",
   "metadata": {},
   "source": [
    "# Assignment: Fine-tuning CLIP for Image–Text Retrieval (Part 2)\n",
    "\n",
    "**Name (Student ID):**  \n",
    "1. LI JIARU (A0332008U)\n",
    "\n",
    "2. JIN YINAN (A0327317E)\n",
    "\n",
    "3. SHI YANCHUN (A0328710J)\n",
    "\n",
    "4. XIAO XIAO (A0332142W)\n",
    "\n",
    "**Total Marks: 25**  \n",
    "**Rubric:** Each step shows its mark weight. Auto-check cells provide partial verification. Keep the notebook runnable on a personal computer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f9f215",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "In Part 1, CLIP was used as a fixed feature extractor. In this part, you will **fine-tune** CLIP for image–text retrieval on a small paired dataset (**Flickr8k**) using a **parameter-efficient** approach. We will **freeze** the image and text encoders and train only the **projection layers**.\n",
    "\n",
    "**Tasks**\n",
    "- Load and split Flickr8k, wrap it with a PyTorch `Dataset` and `DataLoader`.\n",
    "- Implement two contrastive objectives: **InfoNCE** and **Triplet Loss**.\n",
    "- Fine-tune only the projection layers and compare with a **zero-shot** baseline.\n",
    "- Evaluate with **Recall@K** (I2T and T2I).\n",
    "- Run small experiments and answer short questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5cdc81",
   "metadata": {},
   "source": [
    "> **Note**  \n",
    "> This notebook is designed to be runnable on a typical laptop. If memory is tight, reduce `batch_size` and/or the number of training epochs. Avoid large image sizes. Keep the encoders frozen to limit compute.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf6ac65",
   "metadata": {},
   "source": [
    "### Step 1: Setup and Imports (Provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "405e3677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Provided: core imports\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2a435e",
   "metadata": {},
   "source": [
    "### Step 2: Load CLIP and Enable PEFT [3 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d96b5bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 151,277,313\n",
      "Trainable parameters: 655,361 (0.43%)\n"
     ]
    }
   ],
   "source": [
    "model_str = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "# TODO: Load model and processor (similar to Part 1)\n",
    "# Hints: use CLIPModel.from_pretrained / CLIPProcessor.from_pretrained\n",
    "# Move model to device.\n",
    "model = CLIPModel.from_pretrained(model_str).to(device)\n",
    "processor = CLIPProcessor.from_pretrained(model_str)\n",
    "\n",
    "# TODO: Freeze text and vision encoders; unfreeze only projection layers\n",
    "def setup_peft_model(model: \"CLIPModel\") -> \"CLIPModel\":\n",
    "    \"\"\"Freeze encoders, train only projection layers.\"\"\"\n",
    "    # 1) Freeze encoders\n",
    "    for p in model.text_model.parameters():\n",
    "        p.requires_grad = False\n",
    "    for p in model.vision_model.parameters():\n",
    "        p.requires_grad = False\n",
    "    # 2) Unfreeze model.text_projection and model.visual_projection\n",
    "    for p in model.text_projection.parameters():\n",
    "        p.requires_grad = True\n",
    "    for p in model.visual_projection.parameters():\n",
    "        p.requires_grad = True\n",
    "    # 3) Return model\n",
    "    return model\n",
    "\n",
    "# After loading model and processor above, call:\n",
    "model = setup_peft_model(model)\n",
    "\n",
    "# Diagnostics (will be used by auto-checks below)\n",
    "def count_params(model):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable, 100.0 * trainable / max(1, total)\n",
    "\n",
    "# Uncomment after you implement:\n",
    "total, trainable, pct = count_params(model)\n",
    "print(f\"Total parameters: {total:,}\")\n",
    "print(f\"Trainable parameters: {trainable:,} ({pct:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cc8f7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2 check passed.\n"
     ]
    }
   ],
   "source": [
    "# === Auto-check: Step 2 ===\n",
    "try:\n",
    "    assert model is not None and processor is not None, \"Model/processor not initialized.\"\n",
    "    # encoders must be frozen\n",
    "    enc_frozen = all(not p.requires_grad for p in model.text_model.parameters()) and all(not p.requires_grad for p in model.vision_model.parameters())\n",
    "    assert enc_frozen, \"Encoders should be frozen.\"\n",
    "    # projections must be trainable\n",
    "    assert any(p.requires_grad for p in model.text_projection.parameters()), \"text_projection should be trainable.\"\n",
    "    assert any(p.requires_grad for p in model.visual_projection.parameters()), \"visual_projection should be trainable.\"\n",
    "    print(\"Step 2 check passed.\")\n",
    "except Exception as e:\n",
    "    print(\"Step 2 check failed:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bc885f",
   "metadata": {},
   "source": [
    "### Step 3: Data Preparation (Flickr8k) [4 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c761826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the Flickr8k dataset and split into train/test indices\n",
    "# Use the split sizes: 6000 for training, remaining for test (~1000).\n",
    "# Keep a fixed random seed for reproducibility.\n",
    "# Hints: flickr = load_dataset(\"Naveengo/flickr8k\") ; all_data = flickr[\"train\"]\n",
    "flickr = load_dataset(\"Naveengo/flickr8k\")\n",
    "all_data = flickr[\"train\"]\n",
    "num_train = 6000\n",
    "all_indices = list(range(len(all_data)))\n",
    "random.shuffle(all_indices)\n",
    "train_indices = all_indices[:num_train]\n",
    "test_indices = all_indices[num_train:]\n",
    "\n",
    "# TODO: Implement a custom Dataset that packs pixel_values, input_ids, attention_mask\n",
    "class FlickrDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, indices, processor):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.indices = indices\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.hf_dataset[self.indices[idx]]\n",
    "        out = self.processor(images = ex[\"image\"], text = ex[\"text\"],\n",
    "                             padding = \"max_length\", truncation = True, return_tensors = \"pt\")\n",
    "        return {k: v.squeeze(0) for k, v in out.items()}\n",
    "\n",
    "# TODO: Create train_dataset/test_dataset and DataLoaders (batch_size <= 32)\n",
    "train_dataset = FlickrDataset(all_data, train_indices, processor)\n",
    "test_dataset = FlickrDataset(all_data, test_indices, processor)\n",
    "train_loader = DataLoader(train_dataset, batch_size = 32, shuffle = True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 32, shuffle = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29a69163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3 check passed.\n"
     ]
    }
   ],
   "source": [
    "# === Auto-check: Step 3 ===\n",
    "try:\n",
    "    assert train_dataset is not None and test_dataset is not None, \"Datasets not built.\"\n",
    "    assert train_loader is not None and test_loader is not None, \"DataLoaders not built.\"\n",
    "    # quick sample\n",
    "    sample = next(iter(train_loader))\n",
    "    for k in (\"pixel_values\",\"input_ids\",\"attention_mask\"):\n",
    "        assert k in sample, f\"Missing key in batch: {k}\"\n",
    "    bs = sample[\"pixel_values\"].shape[0]\n",
    "    assert bs >= 1, \"Empty batch.\"\n",
    "    print(\"Step 3 check passed.\")\n",
    "except Exception as e:\n",
    "    print(\"Step 3 check failed:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdc0174",
   "metadata": {},
   "source": [
    "### Step 4: Contrastive Losses and Recall@K [7 marks: InfoNCE 3m, Triplet 3m, Recall@K: 1m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc87c9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement InfoNCE loss\n",
    "def info_nce_loss(image_features: torch.Tensor, text_features: torch.Tensor, temperature: float = 0.07):\n",
    "    \"\"\"Return scalar loss.\"\"\"\n",
    "    image_features = F.normalize(image_features, dim = 1)\n",
    "    text_features = F.normalize(text_features, dim = 1)\n",
    "    logits = image_features @ text_features.T / temperature\n",
    "    labels = torch.arange(image_features.size(0), device = logits.device)\n",
    "    return (F.cross_entropy(logits, labels) + F.cross_entropy(logits.T, labels)) / 2\n",
    "\n",
    "# TODO: Implement simple triplet loss with cosine distance and a rolled negative\n",
    "def triplet_loss(image_features: torch.Tensor, text_features: torch.Tensor, margin: float = 0.2):\n",
    "    image_features = F.normalize(image_features, dim = 1)\n",
    "    text_features = F.normalize(text_features, dim = 1)\n",
    "    neg = torch.roll(text_features, shifts = 1, dims = 0)\n",
    "    return F.triplet_margin_with_distance_loss(anchor = image_features, positive = text_features,\n",
    "        negative = neg, distance_function = lambda x, y: 1.0 - F.cosine_similarity(x, y), margin = margin)\n",
    "\n",
    "# TODO: Implement Recall@K for I2T and T2I in a light-weight way (no gradients)\n",
    "@torch.no_grad()\n",
    "def calculate_recall_at_k(model, test_dataset, device, k_values=(1,5,10)):\n",
    "    \"\"\"Return a dict with keys: I2T_R@1, I2T_R@5, I2T_R@10, T2I_R@1, T2I_R@5, T2I_R@10.\"\"\"\n",
    "    model.eval()\n",
    "    I2C = defaultdict(list)\n",
    "    images = {}\n",
    "    for i in test_dataset.indices:\n",
    "        item = test_dataset.hf_dataset[i]\n",
    "        I2C[i].append(item[\"text\"])\n",
    "        images.setdefault(i, item[\"image\"])\n",
    "    image_embeds = torch.cat([model.get_image_features(**processor(images = i, return_tensors = \"pt\").to(device))\n",
    "                              for i in images.values()])\n",
    "    image_embeds = F.normalize(image_embeds, dim = -1)\n",
    "    texts = [c for idx in images for c in I2C[idx]]\n",
    "    text_embeds = model.get_text_features(**processor(text = texts, padding = True, truncation = True, return_tensors = \"pt\").to(device))\n",
    "    text_embeds = F.normalize(text_embeds, dim=-1)\n",
    "    T2I = [i for i, idx in enumerate(images) for _ in range(len(I2C[idx]))]\n",
    "    n_i = image_embeds.size(0)\n",
    "    tpi = [[] for _ in range(n_i)]\n",
    "    for c, i in enumerate(T2I):\n",
    "        tpi[i].append(c)\n",
    "    sim = image_embeds @ text_embeds.T\n",
    "    metrics = {}\n",
    "    for k in k_values:\n",
    "        topk = torch.topk(sim, k, dim = 1).indices\n",
    "        correct = sum(bool(set(topk[i].tolist()) & set(tpi[i])) for i in range(n_i))\n",
    "        metrics[f\"I2T_R@{k}\"] = correct / n_i\n",
    "    n_t = sim.T.size(0)\n",
    "    for k in k_values:\n",
    "        topk = torch.topk(sim.T, k, dim = 1).indices\n",
    "        correct = sum(T2I[t] in set(topk[t].tolist()) for t in range(n_t))\n",
    "        metrics[f\"T2I_R@{k}\"] = correct / n_t\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de37a3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss functions basic shape check passed.\n"
     ]
    }
   ],
   "source": [
    "# === Auto-check: Step 4 ===\n",
    "try:\n",
    "    # create random embeddings to sanity-check loss shapes\n",
    "    a = torch.randn(8, 512)\n",
    "    b = torch.randn(8, 512)\n",
    "    l1 = info_nce_loss(a, b)\n",
    "    l2 = triplet_loss(a, b)\n",
    "    assert torch.is_tensor(l1) and l1.ndim == 0, \"InfoNCE must return a scalar tensor.\"\n",
    "    assert torch.is_tensor(l2) and l2.ndim == 0, \"Triplet loss must return a scalar tensor.\"\n",
    "    print(\"Loss functions basic shape check passed.\")\n",
    "except Exception as e:\n",
    "    print(\"Step 4 loss checks failed:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1c5c9a",
   "metadata": {},
   "source": [
    "### Step 5: Trainer [4 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abdc8fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement a small trainer that accepts a loss function\n",
    "class RetrievalTrainer:\n",
    "    def __init__(self, model, train_loader, test_dataset, device):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.test_dataset = test_dataset\n",
    "        self.device = device\n",
    "        self.writer = None\n",
    "\n",
    "    def train(self, epochs: int, lr: float, loss_fn, loss_name: str):\n",
    "        # Hints: Use AdamW over trainable params only; normalize embeddings; log scalar loss each epoch\n",
    "        optim = torch.optim.AdamW(filter(lambda p: p.requires_grad, self.model.parameters()), lr = lr)\n",
    "        self.writer = SummaryWriter()\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0.0\n",
    "            for batch in self.train_loader:\n",
    "                pixel_values = batch[\"pixel_values\"].to(self.device)\n",
    "                input_ids = batch[\"input_ids\"].to(self.device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "                image_features = self.model.get_image_features(pixel_values = pixel_values)\n",
    "                text_features = self.model.get_text_features(input_ids = input_ids, attention_mask = attention_mask)\n",
    "                loss = loss_fn(image_features, text_features)\n",
    "                optim.zero_grad()\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "                total_loss += loss.item() * pixel_values.size(0)\n",
    "            avg_loss = total_loss / len(self.train_loader.dataset)\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {avg_loss:.4f}\")\n",
    "            self.writer.add_scalar(f\"Loss/{loss_name}\", avg_loss, epoch + 1)\n",
    "            self.evaluate(epoch + 1)\n",
    "        self.writer.close()\n",
    "\n",
    "    def evaluate(self, epoch: int):\n",
    "        # Hints: call calculate_recall_at_k and print metrics; log to TensorBoard as well\n",
    "        metrics = calculate_recall_at_k(self.model, self.test_dataset, self.device)\n",
    "        for k, v in metrics.items():\n",
    "            self.writer.add_scalar(f\"Metrics/{k}\", v, epoch)\n",
    "            print(f\"{k}: {v:.4f}\")\n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9a6ee20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5 structure check passed.\n"
     ]
    }
   ],
   "source": [
    "# === Auto-check: Step 5 (structure only) ===\n",
    "try:\n",
    "    t = RetrievalTrainer\n",
    "    assert callable(getattr(t, \"train\")), \"train() missing.\"\n",
    "    assert callable(getattr(t, \"evaluate\")), \"evaluate() missing.\"\n",
    "    print(\"Step 5 structure check passed.\")\n",
    "except Exception as e:\n",
    "    print(\"Step 5 check failed:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f91379",
   "metadata": {},
   "source": [
    "### Step 6: Experiments [Partial Provided, 2 marks for completion]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4b1caa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Calculating Zero-Shot Baseline Performance ---\n",
      "I2T_R@1: 0.4653\n",
      "I2T_R@5: 0.7303\n",
      "I2T_R@10: 0.8183\n",
      "T2I_R@1: 0.4558\n",
      "T2I_R@5: 0.7044\n",
      "T2I_R@10: 0.8101\n"
     ]
    }
   ],
   "source": [
    "# Baseline: zero-shot metrics without fine-tuning\n",
    "# Note: This will be slower on CPU. Consider reducing test set size temporarily for a quick smoke test.\n",
    "print(\"--- Calculating Zero-Shot Baseline Performance ---\")\n",
    "try:\n",
    "    baseline_metrics = calculate_recall_at_k(model, test_dataset, device)\n",
    "    for k, v in baseline_metrics.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "except Exception as e:\n",
    "    print(\"Baseline evaluation failed (ok if you have not implemented calculate_recall_at_k yet):\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94f15468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.3476\n",
      "I2T_R@1: 0.4400\n",
      "I2T_R@5: 0.7241\n",
      "I2T_R@10: 0.8097\n",
      "T2I_R@1: 0.4720\n",
      "T2I_R@5: 0.7542\n",
      "T2I_R@10: 0.8326\n",
      "Epoch 2, Loss: 0.9132\n",
      "I2T_R@1: 0.4472\n",
      "I2T_R@5: 0.7288\n",
      "I2T_R@10: 0.8173\n",
      "T2I_R@1: 0.4720\n",
      "T2I_R@5: 0.7504\n",
      "T2I_R@10: 0.8350\n",
      "Epoch 3, Loss: 0.7118\n",
      "I2T_R@1: 0.4538\n",
      "I2T_R@5: 0.7293\n",
      "I2T_R@10: 0.8221\n",
      "T2I_R@1: 0.4720\n",
      "T2I_R@5: 0.7465\n",
      "T2I_R@10: 0.8374\n"
     ]
    }
   ],
   "source": [
    "# Experiment 1: InfoNCE fine-tuning\n",
    "# Reset model to fresh PEFT state\n",
    "try:\n",
    "    model = CLIPModel.from_pretrained(model_str).to(device)\n",
    "    model = setup_peft_model(model)\n",
    "    trainer = RetrievalTrainer(model, train_loader, test_dataset, device)\n",
    "    learning_rate = 1e-5\n",
    "    num_epochs = 3  # keep small for resource limits\n",
    "    trainer.train(epochs=num_epochs, lr=learning_rate, loss_fn=info_nce_loss, loss_name=\"InfoNCE\")\n",
    "except Exception as e:\n",
    "    print(\"InfoNCE experiment not executed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "422c74c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.0373\n",
      "I2T_R@1: 0.4543\n",
      "I2T_R@5: 0.7250\n",
      "I2T_R@10: 0.8140\n",
      "T2I_R@1: 0.4605\n",
      "T2I_R@5: 0.7255\n",
      "T2I_R@10: 0.8216\n",
      "Epoch 2, Loss: 0.0206\n",
      "I2T_R@1: 0.4529\n",
      "I2T_R@5: 0.7274\n",
      "I2T_R@10: 0.8211\n",
      "T2I_R@1: 0.4601\n",
      "T2I_R@5: 0.7322\n",
      "T2I_R@10: 0.8197\n",
      "Epoch 3, Loss: 0.0156\n",
      "I2T_R@1: 0.4596\n",
      "I2T_R@5: 0.7331\n",
      "I2T_R@10: 0.8274\n",
      "T2I_R@1: 0.4586\n",
      "T2I_R@5: 0.7303\n",
      "T2I_R@10: 0.8168\n"
     ]
    }
   ],
   "source": [
    "# Experiment 2: Triplet fine-tuning\n",
    "try:\n",
    "    model = CLIPModel.from_pretrained(model_str).to(device)\n",
    "    model = setup_peft_model(model)\n",
    "    trainer = RetrievalTrainer(model, train_loader, test_dataset, device)\n",
    "    learning_rate = 1e-5\n",
    "    num_epochs = 3\n",
    "    trainer.train(epochs=num_epochs, lr=learning_rate, loss_fn=triplet_loss, loss_name=\"Triplet\")\n",
    "except Exception as e:\n",
    "    print(\"Triplet experiment not executed:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87f0704",
   "metadata": {},
   "source": [
    "### Step 7: Analysis[5 marks]\n",
    "\n",
    "**Your Task:** After completing the code above, answer the following analysis questions. *Write your answers in the Markdown cell below AND in the PDF report.*\n",
    "\n",
    "1. **Performance Comparison:** Compare your fine-tuned results (InfoNCE vs Triplet) with the zero-shot baseline. Which improved Recall@K more on Flickr8k and why?  \n",
    "2. **Convergence:** Show training loss plots (TensorBoard) and discuss which objective converged more smoothly.  \n",
    "3. **Learning Rate Sweep:** Try `1e-4` and `1e-6` for InfoNCE. Summarize changes in Recall@K and loss curves.  \n",
    "4. **Overfitting:** Train longer with the better setup and discuss if/when overfitting arises.  \n",
    "5. **Qualitative:** One success and one failure case. Speculate on causes.\n",
    "\n",
    "**(Your analysis goes here)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca006bdd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
