{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "This notebook demonstrates the equivalence of ML estimation and linear regression\n",
    "\n",
    "It also justifies MSE as a loss function for linear regression\n",
    "\n",
    "We would do other loss functions in the tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equivalence between Maximum Likelihood and MSE in Linear Regression\n",
    "\n",
    "In linear regression, we assume a linear relationship between input features $\\mathbf{x}$ and the target variable $y$, with normally distributed errors.\n",
    "\n",
    "Consider the dataset $\\mathcal{D} = [(\\mathbf{x}^{(1)}, y^{(1)}), (\\mathbf{x}^{(2)}, y^{(2)}), \\ldots, (\\mathbf{x}^{(m)}, y^{(m)})] $.\n",
    "\n",
    "- The target variable $ y^{(i)} $ can be written as:\n",
    "     $ y^{(i)} = \\mathbf{w}^T \\mathbf{x}^{(i)} + b + \\epsilon^{(i)} $\n",
    "   - The errors $ \\epsilon^{(i)} $ are i.i.d. and normally distributed: $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$; and it accounts for the residual error of regression\n",
    "\n",
    "- The joint probability of observing the dataset $ \\mathcal{D}  $ is:\n",
    "\n",
    "    $$ P(\\mathcal{D} ) = P\\left((\\mathbf{x}^{(1)},y^{(1)} ), (\\mathbf{x}^{(2)},y^{(2)}), \\ldots, (\\mathbf{x}^{(m)},y^{(m)})\\right) $$\n",
    "\n",
    "- Assuming independent and identifically distributed samples \n",
    "   $$ P(\\mathcal{D} ) =  \\prod_{i=1}^{m}P((\\mathbf{x}^{(i)},y^{(i)})) $$\n",
    "\n",
    "- Using the chain rule, the conditional probability is:\n",
    "     $$ P(\\mathcal{D} ) = \\prod_{i=1}^{m} P(y^{(i)} | \\mathbf{x}^{(i)})P(\\mathbf{x}^{(i)}) $$\n",
    "\n",
    "- The likelihood $ L(\\mathbf{w}, b) $ is:\n",
    "     $$ L(\\mathbf{w}, b) = \\prod_{i=1}^{m} P(y^{(i)} | \\mathbf{x}^{(i)}; \\mathbf{w}, b) $$\n",
    "\n",
    "- For each term, using normal distribution:\n",
    "     $$ P(y^{(i)} | \\mathbf{x}^{(i)}; \\mathbf{w}, b) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y^{(i)} - (\\mathbf{w}^T \\mathbf{x}^{(i)} + b))^2}{2\\sigma^2}\\right)  $$\n",
    "\n",
    "- Taking the natural log of the likelihood:\n",
    "   \n",
    "     $$\\ell(\\mathbf{w}, b) = \\sum_{i=1}^{m} \\left( -\\frac{1}{2} \\log(2\\pi\\sigma^2) - \\frac{(y^{(i)} - (\\mathbf{w}^T \\mathbf{x}^{(i)} + b))^2}{2\\sigma^2} \\right) $$\n",
    "\n",
    "- Ignoring constants for optimization:\n",
    "     $$\\ell(\\mathbf{w}, b) \\propto -\\sum_{i=1}^{m} (y^{(i)} - (\\mathbf{w}^T \\mathbf{x}^{(i)} + b))^2 $$\n",
    "\n",
    "- **Mean Squared Error (MSE)** is defined as:\n",
    "\n",
    "     $$ \\text{MSE}(\\mathbf{w}, b) = \\frac{1}{m} \\sum_{i=1}^{m} (y^{(i)} - (\\mathbf{w}^T \\mathbf{x}^{(i)} + b))^2 $$\n",
    "\n",
    "\n",
    "- Minimizing MSE is equivalent to maximizing the log-likelihood:\n",
    "     $$  \\min_{\\mathbf{w}, b} \\text{MSE}(\\mathbf{w}, b) \\Leftrightarrow \\max_{\\mathbf{w}, b} \\ell(\\mathbf{w}, b) $$\n",
    "\n",
    "This derivation also justifies the MSE loss to train the parameters of linear regression model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
