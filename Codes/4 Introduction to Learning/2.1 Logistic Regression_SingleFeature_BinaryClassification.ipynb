{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses Iris Dataset to demonstrate logistic regression with single feature\n",
    "\n",
    "An interesting article on Iris dataset:\n",
    "https://academic.oup.com/jrssig/article/18/6/26/7038520?login=false\n",
    "\n",
    "- Iris dataset has four features: \n",
    "\n",
    "    - sepal length (cm)  \n",
    "    - sepal width (cm)  \n",
    "    - petal length (cm)  \n",
    "    - petal width (cm) \n",
    "\n",
    "- It has three classes:\n",
    "    - Class 0: Iris-setosa\n",
    "    - Class 1: Iris-versicolor\n",
    "    - Class 2: Iris-virginica\n",
    "\n",
    "We construct a new dataset with single feature and two classes\n",
    "\n",
    "- Feature    \n",
    "    - petal width (cm)\n",
    "\n",
    "- Classes\n",
    "    - Class 1: Iris-virginica \n",
    "    - Class 0: Others (Iris-versicolor and Iris-setosa)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import datasets\n",
    "import seaborn as sns\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "# Specify the file name of the image\n",
    "image_file = 'iris.jpg'\n",
    "# Use the Image class to load and display the image\n",
    "print('Iris virginica, versicolor, and setosa')\n",
    "display(Image(filename=image_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading  the Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "iris_df['species'] = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris_df.head())\n",
    "print(f\"Feature names in iris datset: {iris.feature_names}\")\n",
    "for i, name in enumerate(iris.target_names):\n",
    "    print(f\"Label {i}: {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buildinhg a single feature (petal width) and binaty class dataset with Iris-verginica as class 1 and others as class 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = iris[\"data\"][:, 3:]  # Petal Width \n",
    "y_virginica = (iris[\"target\"] == 2).astype(int)  # 1 if Iris-Virginica, else 0'\n",
    "\n",
    "# Add bias term (intercept term) to the feature matrix\n",
    "X = np.concatenate((np.ones((x_1.shape[0], 1)), x_1), axis = 1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x_1[y_virginica == 0], np.zeros_like(x_1[y_virginica == 0]), s=50, c=\"b\", marker=\"s\", label=\"Not Iris-Virginica\")\n",
    "plt.scatter(x_1[y_virginica == 1], np.zeros_like(x_1[y_virginica == 1]), s=50, c=\"g\", marker=\"^\", label=\"Iris-Virginica\")\n",
    "plt.xlabel(\"Standardized Petal Width\")\n",
    "plt.ylabel(\"Class\")\n",
    "plt.title(\"Standardized Data: Petal Width vs. Class\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for binary classification using logistic regression\n",
    "def logistic_function(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def binary_cross_entropy_loss(y_true, y_pred):\n",
    "    m = len(y_true)\n",
    "    return -(1 / m )* np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-Batch Gradient Descent function\n",
    "def mini_batch_gradient_descent(X, y, learning_rate=0.01, n_iterations=1000, batch_size=10):\n",
    "    m, n = X.shape\n",
    "    w = np.random.randn(n, 1)  # Random initialization\n",
    "    print(w)\n",
    "    loss_history = []\n",
    "    frames = []\n",
    "\n",
    "    for iteration in range(n_iterations):\n",
    "        shuffled_indices = np.random.permutation(m)\n",
    "        X_shuffled = X[shuffled_indices]\n",
    "        y_shuffled = y[shuffled_indices].reshape(-1, 1)\n",
    "        \n",
    "        for i in range(0, m, batch_size):\n",
    "            xi = X_shuffled[i:i+batch_size]\n",
    "            yi = y_shuffled[i:i+batch_size]\n",
    "            y_pred = logistic_function(xi.dot(w))\n",
    "            gradients = (1 / batch_size) * xi.T.dot(y_pred - yi)\n",
    "            w = w - learning_rate * gradients\n",
    "            \n",
    "        y_pred_full = logistic_function(X.dot(w))\n",
    "        loss = binary_cross_entropy_loss(y.reshape(-1, 1), y_pred_full)\n",
    "        loss_history.append(loss)\n",
    "        frames.append((w.copy(), loss_history.copy()))\n",
    "    return w, loss_history, frames\n",
    "\n",
    "# Train the model using mini-batch gradient descent\n",
    "learning_rate = 0.1\n",
    "n_iterations = 300  \n",
    "batch_size = int(X.shape[0])        #int(X.shape[0]) - for full-batch GD   #int(X.shape[0]/10) with 10 batches   #1 for SGD\n",
    "w, loss_history, frames = mini_batch_gradient_descent(X, y_virginica, learning_rate, n_iterations, batch_size)\n",
    "print(w)\n",
    "\n",
    "print(loss_history)\n",
    "# Plot the convergence of the loss function\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(len(loss_history)), loss_history, label='Training Loss', color='blue')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cross-Entropy Loss')\n",
    "plt.title('Convergence of Mini-Batch Gradient Descent')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "print()\n",
    "print(f'Final loss:{loss_history[-1]}')\n",
    "print(f'Final weights:{w.ravel()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the animation embed limit\n",
    "import matplotlib\n",
    "matplotlib.rcParams['animation.embed_limit'] = 300  # Increase limit to 100 MB\n",
    "# Plotting setup\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
    "# Create error surface\n",
    "w0_vals = np.linspace(-4, 4, 100)\n",
    "w1_vals = np.linspace(-4, 4, 100)\n",
    "W0, W1 = np.meshgrid(w0_vals, w1_vals)\n",
    "loss_surface = np.zeros(W0.shape)\n",
    "for i in range(W0.shape[0]):\n",
    "    for j in range(W0.shape[1]):\n",
    "        w = np.array([[W0[i, j]], [W1[i, j]]])\n",
    "        y_pred = logistic_function(X.dot(w))\n",
    "        loss_surface[i, j] = binary_cross_entropy_loss(y_virginica.reshape(-1, 1), y_pred)\n",
    "\n",
    "# Animation function\n",
    "def update(frame_idx):\n",
    "    w, loss_history = frames[frame_idx]\n",
    "\n",
    "    # Decision boundary plot\n",
    "    ax1.clear()\n",
    "    ax1.scatter(x_1[y_virginica == 0], np.zeros_like(x_1[y_virginica == 0]), s=50, c=\"b\", marker=\"s\", label=\"Not Iris-Virginica\")\n",
    "    ax1.scatter(x_1[y_virginica == 1], np.zeros_like(x_1[y_virginica == 1]), s=50, c=\"g\", marker=\"^\", label=\"Iris-Virginica\")\n",
    "    threshold = -(w[0] / w[1]) \n",
    "    ax1.axvline(x=threshold, color='r', linestyle='--', label='Decision Boundary')\n",
    "    ax1.set_xlabel(\"Petal Width (cm)\")\n",
    "    ax1.set_title(r\"Decision Boundary = $-\\frac{w_0}{w_1}$\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Loss convergence plot\n",
    "    ax2.clear()\n",
    "    ax2.plot(range(len(loss_history)), loss_history, label='Training Loss', color='blue')\n",
    "    ax2.set_xlabel('Iterations')\n",
    "    ax2.set_ylabel('Cross-Entropy Loss')\n",
    "    ax2.set_title('Convergence of Mini-Batch Gradient Descent')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "\n",
    "    # Gradient descent path plot with contours\n",
    "    ax3.clear()\n",
    "    ax3.contour(W0, W1, loss_surface, levels=30, cmap='viridis')\n",
    "    ax3.plot([f[0][0, 0] for f in frames[:frame_idx]], [f[0][1, 0] for f in frames[:frame_idx]], 'ro-', markersize=2)\n",
    "    ax3.set_xlabel('w0')\n",
    "    ax3.set_ylabel('w1')\n",
    "    ax3.set_title('Gradient Descent Path with Contours')\n",
    "    ax3.grid(True)\n",
    "\n",
    "# Create animation\n",
    "anim = FuncAnimation(\n",
    "    fig,\n",
    "    update,\n",
    "    frames=len(frames),\n",
    "    repeat=False,\n",
    "    blit=False,\n",
    "    cache_frame_data=False\n",
    ")\n",
    "\n",
    "# Display the animation in the notebook\n",
    "HTML(anim.to_jshtml())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
