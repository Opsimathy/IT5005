{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook complements the lecture slides on backpropagation.\n",
    "\n",
    "The recursion for backpropagation and the corresponding gradient calculations are implemented using Numpy.\n",
    "\n",
    "The results are validated by comparing them with PyTorch based gradient calculations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for nonlinear activations and their derivatives\n",
    "# Only sigmoid and Relu are implemented; but can be extended to other activations\n",
    "# Activation functions and their derivatives\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    # Calculate the sigmoid output\n",
    "    sig = sigmoid(x)\n",
    "    # Use the output to calculate the derivative\n",
    "    return sig * (1 - sig)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters\n",
    "\n",
    "- W contains  L weight matrices one for each layer\n",
    "\n",
    "    -  Ex: for a three layer network, W = [W1, W2, W3] where $Wl$ is the weight matrix at $l$-th layer, i.e., $W^{[l]}$\n",
    "\n",
    "- activations contain the activations from input layer to output layer\n",
    "\n",
    "    - Ex: for a three layer network, \n",
    "    \n",
    "        activations = [a0, a1, a2, a3],\n",
    "        \n",
    "        where $al = \\mathbf{a}^{[l]}$, 'a0' is the input ($\\mathbf{a}^{[0]}$), and 'a3' is the output ($\\mathbf{a}^{[3]}$)\n",
    "\n",
    "- linear_values stores the output of linear function for each layer\n",
    "\n",
    "    - Ex: for a three layer network,\n",
    "        linear_values = [f1, f2, f3] \n",
    "        \n",
    "        where $fl = \\mathbf{f}^{[l]}$ is the output of linear function at $l$-th layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass without bias\n",
    "def forward_pass(W, x, activation_func):\n",
    "    activations = [x]\n",
    "    linear_values = []\n",
    "    for l in range(len(W)):\n",
    "        f = np.dot(W[l].T, activations[-1])\n",
    "        g = activation_func(f)\n",
    "        linear_values.append(f)\n",
    "        activations.append(g)\n",
    "        #print(f\"Layer {l+1}: f (linear) = {f.ravel()}, a (activation) = {g.ravel()}\")\n",
    "    return activations, linear_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial \\hat{y}}{\\partial W^{[l]}} =  \\mathbf{a}^{[l-1]} (\\delta^{[l]})^\\top$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\delta^{[l]} = \\left[ \\mathbf{g}'^{[l]}(\\mathbf{f}^{[l]}) \\right] \\circ \\left( W^{[l+1]} \\delta^{[l+1]} \\right)$$\n",
    "\n",
    "The operator '$\\circ$' indicate the Hadamard product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass without bias\n",
    "def backward_pass(W, activations, linear_values, y_true, activation_derivative):\n",
    "    #L: number of layers\n",
    "    L = len(W)\n",
    "    # initialize deltas with zeros; \n",
    "    # one vector for each layer and (L+1) vectors are created to make the implementation consistent with formula\n",
    "    deltas = [None] * (L + 1)\n",
    "    #final activation layer is output\n",
    "    y_hat = activations[-1]\n",
    "\n",
    "    # Delta for the output layer\n",
    "    deltas[L] = 2 * (y_hat - y_true) * activation_derivative(linear_values[-1])\n",
    "\n",
    "    gradients_W = [None] * L\n",
    "\n",
    "    for l in range(L - 1, -1, -1):\n",
    "        if l > 0:\n",
    "            #dg(f)/df -> diagonal elements of this matrix           \n",
    "            g_derivative = activation_derivative(linear_values[l])\n",
    "            #Recursion for delta values\n",
    "            # The operator * indicates Hadamard product\n",
    "            deltas[l] = g_derivative * np.dot(W[l], deltas[l + 1])\n",
    "\n",
    "        \n",
    "        grad_W = np.outer(activations[l], deltas[l + 1])\n",
    "        gradients_W[l] = grad_W\n",
    "\n",
    "    return gradients_W\n",
    "\n",
    "#Simulating a three layer neural network\n",
    "# Two hidden layer and one output layer\n",
    "# Example weights and biases\n",
    "# At hidden layer 1 and 2\n",
    "#W = [[W_11, W_12], [W_21, W_22]]\n",
    "# At output layer\n",
    "#W = [[W_11], [W_21]]\n",
    "W_np = [\n",
    "    np.array([[0.2, 0.5], [0.2, 0.4]]),   #Weights for hidden layer 1\n",
    "    np.array([[0.5, 0.7], [0.6, 0.8]]),   #Weights for hidden layer 2\n",
    "    np.array([[0.9], [1.0]])                #Weights for output layer \n",
    "]\n",
    "\n",
    "#Input is  a vector \n",
    "x_np = np.array([[0], [1]])\n",
    "y_true_np = np.array([[1]])\n",
    "\n",
    "# Forward and backward pass\n",
    "activations_np, linear_values_np = forward_pass(W_np, x_np, relu)\n",
    "gradients_W_np = backward_pass(W_np, activations_np, linear_values_np, y_true_np, relu_derivative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch network \n",
    "class ThreeLayerNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ThreeLayerNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 2, bias=False)\n",
    "        self.fc2 = nn.Linear(2, 2, bias=False)\n",
    "        self.fc3 = nn.Linear(2, 1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        f1 = self.fc1(x)\n",
    "        a1 = torch.relu(f1)\n",
    "        #print(f\"PyTorch Layer 1: f (linear) = {f1.detach().numpy().ravel()}, a (activation) = {a1.detach().numpy().ravel()}\")\n",
    "        \n",
    "        f2 = self.fc2(a1)\n",
    "        a2 = torch.relu(f2)\n",
    "        #print(f\"PyTorch Layer 2: f (linear) = {f2.detach().numpy().ravel()}, a (activation) = {a2.detach().numpy().ravel()}\")\n",
    "        \n",
    "        f3 = self.fc3(a2)\n",
    "        a3 = torch.relu(f3)\n",
    "        #print(f\"PyTorch Layer 3: f (linear) = {f3.detach().numpy().ravel()}, a (activation) = {a3.detach().numpy().ravel()}\")\n",
    "        \n",
    "        return a1, f1, a2, f2, a3, f3\n",
    "\n",
    "# Initialize PyTorch network\n",
    "net = ThreeLayerNet()\n",
    "\n",
    "# Initialize weights for consistency between NumPy and PyTorch\n",
    "with torch.no_grad():\n",
    "    net.fc1.weight.copy_(torch.tensor(W_np[0].T))\n",
    "    net.fc2.weight.copy_(torch.tensor(W_np[1].T))\n",
    "    net.fc3.weight.copy_(torch.tensor(W_np[2].T))\n",
    "\n",
    "\n",
    "x_torch = torch.tensor([[0., 1.]], requires_grad=True)\n",
    "y_true_torch = torch.tensor([[1.]])\n",
    "\n",
    "# PyTorch forward pass\n",
    "a1, f1, a2, f2, a3, f3 = net(x_torch)\n",
    "\n",
    "# Compute loss and backward pass\n",
    "\n",
    "# Select MSE Loss\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "#Calculation of loss\n",
    "loss = criterion(a3, y_true_torch)\n",
    "\n",
    "#Calculation of gradients\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the forward pass with numpy and pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy Layer 1: f (linear) = [0.2 0.4], a (activation) = [0.2 0.4]\n",
      "PyTorch Layer 1: f (linear) = tensor([0.2000, 0.4000], grad_fn=<ViewBackward0>), a (activation) = tensor([0.2000, 0.4000], grad_fn=<ViewBackward0>)\n",
      "Numpy Layer 2: f (linear) = [0.34 0.46], a (activation) = [0.34 0.46]\n",
      "PyTorch Layer 2: f (linear) = tensor([0.3400, 0.4600], grad_fn=<ViewBackward0>), a (activation) = tensor([0.3400, 0.4600], grad_fn=<ViewBackward0>)\n",
      "Numpy Layer 3: f (linear) = [0.766], a (activation) = [0.766]\n",
      "PyTorch Layer 3: f (linear) = tensor([0.3400, 0.4600], grad_fn=<ViewBackward0>), a (activation) = tensor([0.3400, 0.4600], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Numpy Layer 1: f (linear) = {linear_values_np[0].ravel()}, a (activation) = {activations_np[1].ravel()}\")\n",
    "print(f\"PyTorch Layer 1: f (linear) = {f1.ravel()}, a (activation) = {a1.ravel()}\")\n",
    "\n",
    "print(f\"Numpy Layer 2: f (linear) = {linear_values_np[1].ravel()}, a (activation) = {activations_np[2].ravel()}\")\n",
    "print(f\"PyTorch Layer 2: f (linear) = {f2.ravel()}, a (activation) = {a2.ravel()}\")\n",
    "\n",
    "print(f\"Numpy Layer 3: f (linear) = {linear_values_np[2].ravel()}, a (activation) = {activations_np[3].ravel()}\")\n",
    "print(f\"PyTorch Layer 3: f (linear) = {f2.ravel()}, a (activation) = {a2.ravel()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking backpropagation with numpy and pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 gradients for W in Numpy :\n",
      "[[-0.      -0.     ]\n",
      " [-0.5382  -0.62712]]\n",
      "\n",
      "Layer 1 gradients for W in PyTorch:\n",
      "tensor([[ 0.0000,  0.0000],\n",
      "        [-0.5382, -0.6271]])\n",
      "Layer 2 gradients for W in Numpy :\n",
      "[[-0.08424 -0.0936 ]\n",
      " [-0.16848 -0.1872 ]]\n",
      "\n",
      "Layer 2 gradients for W in PyTorch:\n",
      "tensor([[-0.0842, -0.0936],\n",
      "        [-0.1685, -0.1872]])\n",
      "Layer 3 gradients for W in Numpy :\n",
      "[[-0.15912]\n",
      " [-0.21528]]\n",
      "\n",
      "Layer 3 gradients for W in PyTorch:\n",
      "tensor([[-0.1591],\n",
      "        [-0.2153]])\n"
     ]
    }
   ],
   "source": [
    "# Compare gradients with numpy and pytorch\n",
    "print(f\"Layer 1 gradients for W in Numpy :\\n{gradients_W_np[0]}\\n\")\n",
    "print(f\"Layer 1 gradients for W in PyTorch:\\n{net.fc1.weight.grad.T}\")\n",
    "print(f\"Layer 2 gradients for W in Numpy :\\n{gradients_W_np[1]}\\n\")\n",
    "print(f\"Layer 2 gradients for W in PyTorch:\\n{net.fc2.weight.grad.T}\")\n",
    "print(f\"Layer 3 gradients for W in Numpy :\\n{gradients_W_np[2]}\\n\")\n",
    "print(f\"Layer 3 gradients for W in PyTorch:\\n{net.fc3.weight.grad.T}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
