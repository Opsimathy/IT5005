{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip Connections\n",
    "\n",
    "We learned that changing the activation functions and initialization strategies may help in resolving vanishing gradient issue for relatively shallower neural networks. If the network is deeper, then these strategies fail. The solution is skip connection. The objective of this exercise is to understand the role of skip connection in a simple single-neuron-per-layer neural network. \n",
    "\n",
    "A simple three layer single-neuron-per-layer neural network is designed. \n",
    "\n",
    "![Skip Neural Network](SkipNN.jpg)\n",
    "\n",
    "\n",
    "The closed-form expressions for the gradients are defined for this network. Closed-form expressions are also validated by comparing them with PyTorch gradients. \n",
    "\n",
    "Analyze the gradient flows for this network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation in a Single Neuron Per Layer Network with Skip Connections\n",
    "\n",
    "\n",
    "Assume a network where each layer contains a single neuron, and skip connections exist from the input to each subsequent layer:\n",
    "\n",
    "### Forward Pass\n",
    "\n",
    "1. **Input Layer**:\n",
    "   $$\n",
    "   a^{[0]} = x\n",
    "   $$\n",
    "\n",
    "2. **Layer 1**:\n",
    "   $$\n",
    "   f^{[1]} = W^{[1]} a^{[0]}  \n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   a^{[1]} = g^{[1]}(f^{[1]}) + a^{[0]} \\quad \n",
    "   $$\n",
    "\n",
    "where  $a^{[0]}$  is due to the skip connection from Layer 0\n",
    "\n",
    "3. **Layer 2** (with skip connection from Layer 1):\n",
    "   $$\n",
    "   f^{[2]} = W^{[2]} a^{[1]}  \n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   a^{[2]} = g^{[2]}(f^{[2]}) + a^{[1]} \n",
    "   $$\n",
    "\n",
    "   where  $a^{[1]}$  is due to the skip connection from Layer 1\n",
    "\n",
    "4. **Layer 3** (with skip connection from Layer 2):\n",
    "   $$\n",
    "   f^{[3]} = W^{[3]} a^{[2]}  \n",
    "   $$\n",
    "   \n",
    "   $$\n",
    "   a^{[3]} = g^{[3]}(f^{[3]})  + a^{[2]} \n",
    "   $$\n",
    "\n",
    "where  $a^{[2]}$  is due to the skip connection from Layer 2\n",
    "\n",
    "5. **Output Layer**:\n",
    "   $$\n",
    "   \\hat{y} = a^{[3]}\n",
    "   $$\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "Using Mean Squared Error as the loss:\n",
    "\n",
    "$$\n",
    "\\mathcal{E} = \\frac{1}{2} (\\hat{y} - y)^2\n",
    "$$\n",
    "\n",
    "## Gradients \n",
    "\n",
    "For any layer $l$, we need to find $\\frac{\\partial E}{\\partial W^{[l]}}$. Let's start with the output layer and work backwards.\n",
    "\n",
    "### 1. Layer 3\n",
    "The loss gradient with respect to $W^{[3]}$ can be derived as:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial W^{[3]}} = \\frac{\\partial E}{\\partial a^{[3]}} \\cdot \\frac{\\partial a^{[3]}}{\\partial f^{[3]}} \\cdot \\frac{\\partial f^{[3]}}{\\partial W^{[3]}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- From MSE loss: $$\\frac{\\partial E}{\\partial a^{[3]}} = (a^{[3]} - y)$$ \n",
    "- Derivative of activation: $$\\frac{\\partial a^{[3]}}{\\partial f^{[3]}} = g'^{[3]}(f^{[3]})$$ \n",
    "- From $f^{[3]} = W^{[3]}a^{[2]}$: $$\\frac{\\partial f^{[3]}}{\\partial W^{[3]}} = a^{[2]}$$ \n",
    "\n",
    "Therefore:\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial W^{[3]}} = (a^{[3]} - y) \\delta^{[3]} \\cdot a^{[2]}\n",
    "$$\n",
    "where $$\\delta^{[3]} =  g'^{[3]}(f^{[3]})$$\n",
    "\n",
    "### 2. Layer 2\n",
    "\n",
    "The loss gradient with respect to $W^{[2]}$ can be derived as:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial W^{[2]}} = (a^{[3]} - y) \\cdot \\left(\\delta^{[2]}  +  g'^{[2]}(f^{[2]})\\right) \\cdot a^{[1]}\n",
    "$$\n",
    "\n",
    "where $$\\delta^{[2]} = \\delta^{[3]} W^{[3]}  g'^{[2]}(f^{[2]})$$\n",
    "\n",
    "### 3. Layer 1\n",
    "\n",
    "The loss gradient with respect to $W^{[1]}$ can be derived as:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial W^{[1]}} = (a^{[3]} - y) \\cdot \\left(\\delta^{[1]}   + \\delta^{[3]} \\cdot  W^{[3]}  g'^{[1]}(f^{[1]})  + g'^{[2]} (f^{[2]}) W^{[2]} g'^{[1]} (f^{[1]}) + g'^{[1]}(f^{[1]})\\right)\\cdot a^{[0]}\n",
    "$$\n",
    "\n",
    "where $$\\delta^{[1]} = \\delta^{[2]} W^{[2]}  g'^{[1]}(f^{[1]})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network is implemented and gradients are validated via backpropagation using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Pass Values:\n",
      "x (a0): 1.00000000\n",
      "a3 (y_pred): 3.36391068\n",
      "Loss: 4.10099220\n",
      "\n",
      "Manual Gradient Calculation:\n",
      "dw1_manual: 0.75138211\n",
      "dw2_manual: 0.85736593\n",
      "dw3_manual: 0.75415744\n",
      "\n",
      "PyTorch Gradients:\n",
      "dE/dw1: 0.75138211\n",
      "dE/dw2: 0.85736591\n",
      "dE/dw3: 0.75415742\n",
      "\n",
      "Differences in Gradients (PyTorch AutoGrad - Manual):\n",
      "w1 diff: 0.00000001\n",
      "w2 diff: 0.00000002\n",
      "w3 diff: 0.00000001\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the neural network model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.w1 = nn.Parameter(torch.tensor([0.8]))\n",
    "        self.w2 = nn.Parameter(torch.tensor([0.8]))\n",
    "        self.w3 = nn.Parameter(torch.tensor([0.8]))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        f1 = self.w1 * x\n",
    "        a1 = self.sigmoid(f1) + x\n",
    "\n",
    "        f2 = self.w2 * a1\n",
    "        a2 = self.sigmoid(f2) + a1\n",
    "\n",
    "        f3 = self.w3 * a2\n",
    "        a3 = self.sigmoid(f3) + a2\n",
    "\n",
    "        return a3, f1, a1, f2, a2, f3\n",
    "\n",
    "# Instantiate the model\n",
    "model = SimpleNN()\n",
    "\n",
    "# Create input and target tensors\n",
    "x = torch.tensor([1.0])\n",
    "y_true = torch.tensor([0.5])\n",
    "\n",
    "# Forward pass\n",
    "a3, f1, a1, f2, a2, f3 = model(x)\n",
    "\n",
    "# Compute loss\n",
    "# Note in Pytorch, MSE Loss does include the factor 0.5\n",
    "criterion = nn.MSELoss()\n",
    "loss = 0.5*criterion(a3, y_true)\n",
    "\n",
    "print(\"Forward Pass Values:\")\n",
    "print(f\"x (a0): {x.item():.8f}\")\n",
    "print(f\"a3 (y_pred): {a3.item():.8f}\")\n",
    "print(f\"Loss: {loss.item():.8f}\")\n",
    "\n",
    "\n",
    "# Perform backward pass to compute gradients\n",
    "loss.backward()\n",
    "\n",
    "# Manual gradient calculation\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = 1 / (1 + torch.exp(-x))\n",
    "    return s * (1 - s)\n",
    "\n",
    "# Layer 3\n",
    "delta3 = sigmoid_derivative(f3).item()\n",
    "dw3_manual = (a3.item() - y_true.item()) * delta3 * a2.item()\n",
    "\n",
    "# Layer 2\n",
    "delta2 = delta3 * model.w3.item() * sigmoid_derivative(f2).item()\n",
    "dw2_manual = (a3.item() - y_true.item()) * (delta2 * a1.item() + sigmoid_derivative(f2).item() * a1.item())\n",
    "\n",
    "# Layer 1\n",
    "delta1 = delta2 * model.w2.item() * sigmoid_derivative(f1).item()\n",
    "dw1_manual = (a3.item() - y_true.item()) * (\n",
    "    delta1 * x.item() +\n",
    "    delta3 * model.w3.item() * sigmoid_derivative(f1).item() * x.item() +\n",
    "    sigmoid_derivative(f2).item() * model.w2.item() * sigmoid_derivative(f1).item() * x.item() +\n",
    "    sigmoid_derivative(f1).item() * x.item()\n",
    ")\n",
    "\n",
    "print(\"\\nManual Gradient Calculation:\")\n",
    "print(f\"dw1_manual: {dw1_manual:.8f}\")\n",
    "print(f\"dw2_manual: {dw2_manual:.8f}\")\n",
    "print(f\"dw3_manual: {dw3_manual:.8f}\")\n",
    "\n",
    "print(\"\\nPyTorch Gradients:\")\n",
    "print(f\"dE/dw1: {model.w1.grad.item():.8f}\")\n",
    "print(f\"dE/dw2: {model.w2.grad.item():.8f}\")\n",
    "print(f\"dE/dw3: {model.w3.grad.item():.8f}\")\n",
    "\n",
    "print(\"\\nDifferences in Gradients (PyTorch AutoGrad - Manual):\")\n",
    "print(f\"w1 diff: {abs(model.w1.grad.item() - dw1_manual):.8f}\")\n",
    "print(f\"w2 diff: {abs(model.w2.grad.item() - dw2_manual):.8f}\")\n",
    "print(f\"w3 diff: {abs(model.w3.grad.item() - dw3_manual):.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework:\n",
    "\n",
    "Prove the closed-form expressions for the gradients using chain rule."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
