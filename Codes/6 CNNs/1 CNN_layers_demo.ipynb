{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Layers in CNN\n",
    "\n",
    "This notebook demonstrates the working of basic building blocks of CNNs\n",
    "\n",
    "- Convolution Layer\n",
    "- Pooling Layer\n",
    "    - Pooling layers are crucial for reducing the dimensionality of feature maps while retaining essential information. They help control overfitting, reduce computation, and provide translational invariance. Choosing the appropriate pooling strategy depends on the specific needs of the model architecture and the nature of the input data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Module\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "\n",
    "Convolution layer is implemented using convolution module. Key input arguments for the convolution module:\n",
    "\n",
    "- **in_channels**: Number of input channels\n",
    "  - For an RGB image, the number of `in_channels` is 3.\n",
    "  - For an intermediate layer, it is equal to the number of kernels (or filters or feature maps) in the previous layer.\n",
    "\n",
    "- **out_channels**: Indicates the number of filters or feature maps or neurons in each convolution layer.\n",
    "  - This determines how many feature maps will be produced by the convolutional layer\n",
    "  \n",
    "- **kernel_size**: Indicates the size of convolution filters or kernels.\n",
    "  - Specified as a single integer for square kernels (e.g., `3` for a 3x3 filter) or a tuple for non-square kernels (e.g., `(3, 5)` for a 3x5 filter).\n",
    "\n",
    "- **padding**: Indicates the width of additional data augmented around the input.\n",
    "  - Padding is used to control the spatial dimensions of the output.\n",
    "  - The amount of padding often depends on the size of the kernel:\n",
    "    - To maintain the input dimensions at the output (when stride is 1), padding is typically set such that `padding = (kernel_size - 1) / 2` for square kernels.\n",
    "\n",
    "- **stride**: This defines the step size with which the convolutional filter moves across the input. The stride can be specified as a single integer for a uniform stride in all directions or as a tuple for different strides in height and width. A larger stride reduces the spatial dimensions of the output feature map.\n",
    "\n",
    "\n",
    "- **dilation**: The dilation parameter controls the spacing between elements of the kernel. It can be used to expand the field of view of the kernel without increasing the number of parameters. Dilation is specified as a single integer or a tuple, indicating the dilation rate along height and width. A dilation rate of 1 means no dilation.\n",
    "\n",
    "\n",
    "- **groups**: This parameter controls the connections between inputs and outputs. Setting `groups=1` means a standard convolution, where each input channel is convolved with every filter. If `groups` equals the number of `in_channels`, it performs a depthwise convolution, where each input channel is convolved with its own set of filters.\n",
    "\n",
    "\n",
    "- **bias**: This is a boolean parameter that specifies whether to include a learnable bias in the convolution operation. By default, it is set to `True`, meaning that a bias will be added to the output of each filter.\n",
    "\n",
    "\n",
    "- **padding_mode**: This determines the type of padding to use. The default is `'zeros'`, meaning zero-padding, but it can also be set to `'reflect'`, `'replicate'`, or `'circular'` for other types of padding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relation between Convolution parameters\n",
    "\n",
    "For a given spatial dimension (either height $H$ or width $W$):\n",
    "\n",
    "$O = \\left\\lfloor \\frac{I + 2P - K}{S} \\right\\rfloor + 1$\n",
    "\n",
    "Where:\n",
    "- $ O $ is the output dimension (either height or width).\n",
    "- $ I $ is the input dimension (either height or width).\n",
    "- $ P $ is the padding applied to the input.\n",
    "- $ K$ is the kernel size.\n",
    "- $ S $ is the stride of the convolution.\n",
    "- $\\left\\lfloor \\cdot \\right\\rfloor$ denotes the floor operation, which rounds down to the nearest integer.\n",
    "\n",
    "### Example\n",
    "- Input size $ I = 32 $ (e.g., a 32x32 image),\n",
    "- Kernel size $ K = 3 $,\n",
    "- Padding $ P = 1 $,\n",
    "- Stride $ S = 1 $.\n",
    "\n",
    "Plug in these values into the formula:\n",
    "\n",
    "$ O = \\left\\lfloor \\frac{32 + 2 \\times 1 - 3}{1} \\right\\rfloor + 1 = \\left\\lfloor \\frac{32 + 2 - 3}{1} \\right\\rfloor + 1 = \\left\\lfloor \\frac{31}{1} \\right\\rfloor + 1 = 32 $\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of convolution filters: torch.Size([3, 2, 3, 3])\n",
      "Conv Filter Weights\n",
      "Parameter containing:\n",
      "tensor([[[[-0.0868,  0.1588,  0.0101],\n",
      "          [ 0.2002, -0.2348, -0.2257],\n",
      "          [-0.0876,  0.1261, -0.0050]],\n",
      "\n",
      "         [[-0.1209, -0.1802, -0.0792],\n",
      "          [-0.0016,  0.0676,  0.1228],\n",
      "          [-0.1594,  0.1482, -0.1149]]],\n",
      "\n",
      "\n",
      "        [[[-0.1057, -0.0372, -0.2201],\n",
      "          [ 0.1505, -0.2089,  0.0772],\n",
      "          [-0.0564,  0.1741, -0.1507]],\n",
      "\n",
      "         [[-0.0651,  0.0247, -0.0753],\n",
      "          [ 0.1960, -0.2213, -0.1049],\n",
      "          [ 0.0806, -0.1306,  0.0799]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2138, -0.1937, -0.1818],\n",
      "          [-0.1340, -0.0774,  0.0700],\n",
      "          [ 0.0918, -0.0159,  0.1855]],\n",
      "\n",
      "         [[-0.2290,  0.2192,  0.1438],\n",
      "          [ 0.1562,  0.0672, -0.0123],\n",
      "          [ 0.2122,  0.2040,  0.0350]]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#Three kernels each of size 3-by-3-by-2\n",
    "#conv_filters = nn.Conv2d(in_channels=2 , out_channels=3 ,  kernel_size=3,  padding=1 )\n",
    "conv_filters = nn.Conv2d(in_channels=2 , out_channels=3 ,  kernel_size=3,  padding=1 )\n",
    "# Size of convolution filters\n",
    "'''\n",
    "    Output Channels: 3 filters or output feature maps\n",
    "    Input Channels: 2 channels from the input\n",
    "    Kernel Height: 3 pixels\n",
    "    Kernel Width: 3 pixels\n",
    "'''\n",
    "print(f'Size of convolution filters: {conv_filters.weight.size()}')\n",
    "print(\"Conv Filter Weights\")\n",
    "print(conv_filters.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make an input 2 x 3 x 4  (two channels, each has 3 x 4 pixels )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the data: torch.Size([3, 2, 3, 4])\n",
      "tensor([[[[0.0249, 0.5162, 0.2239, 0.7541],\n",
      "          [0.0098, 0.8837, 0.9687, 0.8592],\n",
      "          [0.2849, 0.7420, 0.5915, 0.4588]],\n",
      "\n",
      "         [[0.3153, 0.5290, 0.3056, 0.7309],\n",
      "          [0.6363, 0.2823, 0.6907, 0.2244],\n",
      "          [0.6115, 0.1865, 0.8038, 0.5614]]],\n",
      "\n",
      "\n",
      "        [[[0.1366, 0.5261, 0.0554, 0.1270],\n",
      "          [0.1539, 0.0288, 0.0954, 0.7523],\n",
      "          [0.9725, 0.9499, 0.0919, 0.4983]],\n",
      "\n",
      "         [[0.5989, 0.8211, 0.5218, 0.6614],\n",
      "          [0.9070, 0.1353, 0.0649, 0.9614],\n",
      "          [0.4433, 0.0429, 0.6795, 0.5759]]],\n",
      "\n",
      "\n",
      "        [[[0.6359, 0.9382, 0.9903, 0.0515],\n",
      "          [0.7929, 0.1298, 0.1630, 0.3775],\n",
      "          [0.4837, 0.8303, 0.8130, 0.8790]],\n",
      "\n",
      "         [[0.2229, 0.0761, 0.9712, 0.8842],\n",
      "          [0.7572, 0.1363, 0.7073, 0.9360],\n",
      "          [0.6933, 0.9016, 0.2981, 0.4730]]]])\n"
     ]
    }
   ],
   "source": [
    "batch_size=3\n",
    "input_channels  =2\n",
    "input_height = 3\n",
    "input_width = 4\n",
    "data =torch.rand(batch_size,input_channels,input_height,input_width)\n",
    "print(f\"Dimensions of the data: {data.size()}\")\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feed it to the convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of convolution filters: torch.Size([3, 3, 3, 4])\n",
      "tensor([[[[-0.0919, -0.2418, -0.0524, -0.2513],\n",
      "          [-0.2267, -0.6080, -0.4316, -0.2155],\n",
      "          [-0.4112, -0.2882, -0.1761, -0.1388]],\n",
      "\n",
      "         [[-0.1101,  0.0753,  0.1609,  0.0681],\n",
      "          [-0.2055,  0.0656, -0.3154,  0.2089],\n",
      "          [-0.1850, -0.2332, -0.3409,  0.0252]],\n",
      "\n",
      "         [[ 0.1683,  0.2528,  0.3102,  0.0920],\n",
      "          [ 0.2273,  0.1964,  0.1184,  0.0666],\n",
      "          [-0.0978, -0.4698, -0.2568, -0.2039]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0141, -0.2470, -0.0524,  0.1292],\n",
      "          [-0.0466, -0.3942, -0.5064, -0.3424],\n",
      "          [-0.6714, -0.2250, -0.0258, -0.2452]],\n",
      "\n",
      "         [[-0.1193,  0.0110,  0.2068,  0.1154],\n",
      "          [-0.2686,  0.4150, -0.2015, -0.1374],\n",
      "          [-0.0597,  0.0348, -0.1194,  0.0694]],\n",
      "\n",
      "         [[ 0.0650,  0.1568,  0.1215,  0.1312],\n",
      "          [ 0.2453,  0.2027,  0.3173,  0.0946],\n",
      "          [ 0.0197, -0.4765, -0.2659, -0.0196]]],\n",
      "\n",
      "\n",
      "        [[[-0.2553, -0.5427, -0.0139,  0.1890],\n",
      "          [-0.1418,  0.1050, -0.3560, -0.4008],\n",
      "          [-0.2780, -0.5185, -0.4605, -0.3351]],\n",
      "\n",
      "         [[ 0.0853,  0.1268, -0.2243,  0.2967],\n",
      "          [-0.4547, -0.1278, -0.2020, -0.0289],\n",
      "          [-0.1620, -0.1839,  0.0919,  0.0108]],\n",
      "\n",
      "         [[ 0.0171,  0.0697, -0.0312,  0.2403],\n",
      "          [-0.1017,  0.2327,  0.6868,  0.3318],\n",
      "          [-0.1194, -0.0180,  0.0443, -0.2778]]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output_conv_filters = conv_filters(data)\n",
    "print(f\"Output of convolution filters: {output_conv_filters.size()}\")\n",
    "print(output_conv_filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate Convolutional layer with numpy\n",
    "\n",
    "- helps us in understanding the workings of convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor:\n",
      "tensor([[[[-0.3633, -1.1081, -1.9856,  1.5418, -0.8696],\n",
      "          [ 1.1699,  0.8141,  1.3254, -1.2161, -0.1563],\n",
      "          [ 0.6774,  1.5959,  0.6463,  1.6820,  0.5404],\n",
      "          [-1.1969,  0.8465, -1.6798,  0.0784, -0.6348],\n",
      "          [ 1.2818, -0.0113,  0.2723, -1.4679,  0.7478]]]])\n",
      "\n",
      "Output Tensor:\n",
      "tensor([[[[ 0.2038,  0.9574,  0.1992],\n",
      "          [ 0.5795, -0.8319, -0.0182],\n",
      "          [-0.5165,  0.8156, -1.2960]]]], grad_fn=<ConvolutionBackward0>)\n",
      "\n",
      "Convolution Weights:\n",
      "[[[[-0.02017804 -0.18675594  0.3170899 ]\n",
      "   [ 0.23428199 -0.04005798  0.03704413]\n",
      "   [ 0.14230311  0.31781924 -0.16872013]]]]\n",
      "\n",
      "Convolution Bias:\n",
      "[-0.16598439]\n",
      "\n",
      "Output from NumPy convolution:\n",
      "[[[[ 0.20382187  0.95738828  0.19921386]\n",
      "   [ 0.57949209 -0.83186579 -0.01816052]\n",
      "   [-0.51645017  0.81558001 -1.29595339]]]]\n"
     ]
    }
   ],
   "source": [
    "# Define the convolutional layer\n",
    "conv_filters = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding=0)\n",
    "\n",
    "# Create a random input tensor with shape (batch_size, channels, height, width)\n",
    "#height and width of input should be larger than kernel size\n",
    "#depth of input must match with number of in_channels of convolutional filter\n",
    "input_tensor = torch.randn(1, 1, 5, 5)\n",
    "\n",
    "# Apply the convolutional layer\n",
    "output_tensor = conv_filters(input_tensor)\n",
    "\n",
    "# Print the input tensor\n",
    "print(\"Input Tensor:\")\n",
    "print(input_tensor)\n",
    "\n",
    "# Print the output tensor\n",
    "print(\"\\nOutput Tensor:\")\n",
    "print(output_tensor)\n",
    "\n",
    "# Extract convolution parameters\n",
    "weight = conv_filters.weight.detach().numpy()\n",
    "bias = conv_filters.bias.detach().numpy()\n",
    "\n",
    "print(\"\\nConvolution Weights:\")\n",
    "print(weight)\n",
    "\n",
    "print(\"\\nConvolution Bias:\")\n",
    "print(bias)\n",
    "\n",
    "# Define a function to perform convolution using NumPy\n",
    "def numpy_conv2d(input_data, weight, bias, stride=1, padding=0):\n",
    "    # Assuming input_data shape is (batch_size, channels, height, width)\n",
    "    batch_size, in_channels, in_height, in_width = input_data.shape\n",
    "    out_channels, _, kernel_height, kernel_width = weight.shape\n",
    "\n",
    "    # Output dimensions\n",
    "    out_height = (in_height - kernel_height + 2 * padding) // stride + 1\n",
    "    out_width = (in_width - kernel_width + 2 * padding) // stride + 1\n",
    "    \n",
    "    # Initialize the output\n",
    "    output_data = np.zeros((batch_size, out_channels, out_height, out_width))\n",
    "\n",
    "    # Perform convolution\n",
    "    for b in range(batch_size):\n",
    "        for oc in range(out_channels):\n",
    "            for oh in range(out_height):\n",
    "                for ow in range(out_width):\n",
    "                    # Calculate corresponding input region and apply convolution\n",
    "                    h_start = oh * stride\n",
    "                    h_end = h_start + kernel_height\n",
    "                    w_start = ow * stride\n",
    "                    w_end = w_start + kernel_width\n",
    "\n",
    "                    region = input_data[b, :, h_start:h_end, w_start:w_end]\n",
    "                    output_data[b, oc, oh, ow] = np.sum(region * weight[oc]) + bias[oc]\n",
    "\n",
    "    return output_data\n",
    "\n",
    "# Convert input_tensor to NumPy array\n",
    "input_data_numpy = input_tensor.detach().numpy()\n",
    "\n",
    "# Perform the convolution using NumPy\n",
    "output_numpy = numpy_conv2d(input_data_numpy, weight, bias)\n",
    "\n",
    "print(\"\\nOutput from NumPy convolution:\")\n",
    "print(output_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MaxPool2d Layer\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html\n",
    "\n",
    "The `nn.MaxPool2d` layer is used for downsampling operations in convolutional neural networks. It reduces the spatial dimensions (height and width) of the input while retaining the most salient features. \n",
    "\n",
    "Here are the key parameters and their explanations:\n",
    "\n",
    "- **kernel_size**: Specifies the size of the window to take a max over.\n",
    "  - If a single integer is provided, the window will be square (e.g., `2` for a 2x2 window).\n",
    "  - A tuple can be used for non-square windows (e.g., `(2, 3)` for a window 2 pixels high and 3 pixels wide).\n",
    "\n",
    "- **stride**: Specifies the step size with which the pooling window moves across the input.\n",
    "  - If not specified, it defaults to the value of `kernel_size`.\n",
    "  - A larger stride results in more aggressive downsampling.\n",
    "\n",
    "- **padding**: Adds zero-padding to the input on both sides.\n",
    "  - This can help control the dimensions of the output.\n",
    "  - Typically used less frequently in pooling layers compared to convolutional layers.\n",
    "\n",
    "- **dilation**: Controls the spacing between elements in the pooling window.\n",
    "  - Default is `1`, meaning standard max pooling.\n",
    "  - Larger values can result in a wider effective pooling window.\n",
    "\n",
    "- **return_indices**: If `True`, returns the indices of the max values along with the outputs.\n",
    "  - Useful for operations that require the location of maximum values, such as `nn.MaxUnpool2d`.\n",
    "\n",
    "- **ceil_mode**: If `True`, uses the ceiling function to compute the output shape instead of the floor function.\n",
    "  - Can affect the output size when the input size is not perfectly divisible by the kernel size.\n",
    "\n",
    "For each region defined by the `kernel_size`, the `nn.MaxPool2d` layer selects the maximum value and discards the rest, effectively reducing the size of the input feature map. \n",
    "\n",
    "Think of this as feature selection.\n",
    "\n",
    "By reducing spatial dimensions, pooling layers help decrease computation and control overfitting by reducing the number of parameters.\n",
    "\n",
    "#### Relation between input and output sizes\n",
    "\n",
    "Given an input of size `(H_in, W_in)`, the output dimensions `(H_out, W_out)` can be calculated using the following formulas:\n",
    "\n",
    "   - $ H_{out} = \\left\\lfloor \\frac{H_{in} + 2 \\times \\text{padding}[0] - \\text{kernel\\_size}[0]}{\\text{stride}[0]} \\right\\rfloor + 1 $\n",
    "\n",
    "   - $ W_{out} = \\left\\lfloor \\frac{W_{in} + 2 \\times \\text{padding}[1] - \\text{kernel\\_size}[1]}{\\text{stride}[1]} \\right\\rfloor + 1 $\n",
    "\n",
    "These relationships mean that max pooling reduces the spatial dimensions of the input based on the specified parameters, which can help in reducing the computational load and controlling the overfitting by reducing the number of parameters in the network.\n",
    "\n",
    "\n",
    "\n",
    "#### Example\n",
    "\n",
    "Consider an example with the following parameters:\n",
    "\n",
    "- `kernel_size` = 2\n",
    "- `stride` = 2\n",
    "- Input size: `(1, 3, 32, 32)`    -> here 1 is the batch size, 3 is depth or number of channels, the last two integers are width and height\n",
    "\n",
    "When applied:\n",
    "\n",
    "- The input dimensions of `(32, 32)` are reduced to `(16, 16)`, assuming no padding and a stride equal to the kernel size. maxpool does not change the number of channels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pool_2D = nn.MaxPool2d(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of output of convolution filters: torch.Size([3, 3, 3, 4])\n",
      "Size of output of maxpool: torch.Size([3, 3, 1, 2])\n",
      "Input of maxpool: \n",
      "\n",
      "tensor([[[[-0.0919, -0.2418, -0.0524, -0.2513],\n",
      "          [-0.2267, -0.6080, -0.4316, -0.2155],\n",
      "          [-0.4112, -0.2882, -0.1761, -0.1388]],\n",
      "\n",
      "         [[-0.1101,  0.0753,  0.1609,  0.0681],\n",
      "          [-0.2055,  0.0656, -0.3154,  0.2089],\n",
      "          [-0.1850, -0.2332, -0.3409,  0.0252]],\n",
      "\n",
      "         [[ 0.1683,  0.2528,  0.3102,  0.0920],\n",
      "          [ 0.2273,  0.1964,  0.1184,  0.0666],\n",
      "          [-0.0978, -0.4698, -0.2568, -0.2039]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0141, -0.2470, -0.0524,  0.1292],\n",
      "          [-0.0466, -0.3942, -0.5064, -0.3424],\n",
      "          [-0.6714, -0.2250, -0.0258, -0.2452]],\n",
      "\n",
      "         [[-0.1193,  0.0110,  0.2068,  0.1154],\n",
      "          [-0.2686,  0.4150, -0.2015, -0.1374],\n",
      "          [-0.0597,  0.0348, -0.1194,  0.0694]],\n",
      "\n",
      "         [[ 0.0650,  0.1568,  0.1215,  0.1312],\n",
      "          [ 0.2453,  0.2027,  0.3173,  0.0946],\n",
      "          [ 0.0197, -0.4765, -0.2659, -0.0196]]],\n",
      "\n",
      "\n",
      "        [[[-0.2553, -0.5427, -0.0139,  0.1890],\n",
      "          [-0.1418,  0.1050, -0.3560, -0.4008],\n",
      "          [-0.2780, -0.5185, -0.4605, -0.3351]],\n",
      "\n",
      "         [[ 0.0853,  0.1268, -0.2243,  0.2967],\n",
      "          [-0.4547, -0.1278, -0.2020, -0.0289],\n",
      "          [-0.1620, -0.1839,  0.0919,  0.0108]],\n",
      "\n",
      "         [[ 0.0171,  0.0697, -0.0312,  0.2403],\n",
      "          [-0.1017,  0.2327,  0.6868,  0.3318],\n",
      "          [-0.1194, -0.0180,  0.0443, -0.2778]]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "Output of max pool: \n",
      "\n",
      "tensor([[[[-0.0919, -0.0524]],\n",
      "\n",
      "         [[ 0.0753,  0.2089]],\n",
      "\n",
      "         [[ 0.2528,  0.3102]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0141,  0.1292]],\n",
      "\n",
      "         [[ 0.4150,  0.2068]],\n",
      "\n",
      "         [[ 0.2453,  0.3173]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1050,  0.1890]],\n",
      "\n",
      "         [[ 0.1268,  0.2967]],\n",
      "\n",
      "         [[ 0.2327,  0.6868]]]], grad_fn=<MaxPool2DWithIndicesBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output_max_pool = max_pool_2D(output_conv_filters)\n",
    "print(f\"Size of output of convolution filters: {output_conv_filters.size()}\")\n",
    "print(f\"Size of output of maxpool: {output_max_pool.size()}\")\n",
    "print(\"Input of maxpool: \\n\")\n",
    "print(output_conv_filters)\n",
    "print(\"Output of max pool: \\n\")\n",
    "print(output_max_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Pooling Layers in PyTorch\n",
    "\n",
    "#### 1. Average Pooling (`nn.AvgPool2d`)\n",
    "\n",
    "Instead of taking the maximum value, it computes the average of all values in the defined window. Does a smoother aggregation of featires instead of aggressive selectoon of features via max pooling.\n",
    "\n",
    "- **Parameters**:\n",
    "  - **`kernel_size`**: The size of the window over which the average is computed.\n",
    "  - **`stride`**: Determines the step size of the window. Defaults to `kernel_size` if not specified.\n",
    "  - **`padding`**: Adds zero-padding around the input.\n",
    "  - **`count_include_pad`**: When set to `True`, includes padded zeros in the averaging calculation.\n",
    "  - **`ceil_mode`**: Uses the ceiling function to determine output size if set to `True`.\n",
    "\n",
    "#### 2. Adaptive Pooling (`nn.AdaptiveMaxPool2d`, `nn.AdaptiveAvgPool2d`)\n",
    "\n",
    "Adaptive pooling layers output a fixed size, regardless of the input size. They are particularly useful for creating networks that can handle variable input sizes by producing consistent output dimensions. \n",
    "\n",
    "Used in classification networks, where a fixed-size output is required before fully connected layers.\n",
    "\n",
    "- **`nn.AdaptiveMaxPool2d`**: Similar to max pooling, but adapts output size.\n",
    "- **`nn.AdaptiveAvgPool2d`**: Similar to average pooling, but adapts output size.\n",
    "\n",
    "\n",
    "We need to specify the target output size (height, width). The layer calculates how to split the input dimensions to achieve this fixed size.\n",
    "\n",
    "\n",
    "#### 3. Global Pooling\n",
    "\n",
    "Global pooling layers reduce each feature map to a single value, typically by taking the average or maximum across all spatial dimensions. This is not explicitly provided as a separate class in PyTorch but can be implemented using adaptive pooling with an output size of 1.\n",
    "- Use `nn.AdaptiveAvgPool2d(output_size=(1, 1))`.\n",
    "- Use `nn.AdaptiveMaxPool2d(output_size=(1, 1))`.\n",
    "\n",
    "-  Used in architectures like ResNet to reduce feature maps to a vector before feeding them into the final classification layer. It reduces the model's sensitivity to spatial translations of the input.\n",
    "\n",
    "#### 4. Fractional Max Pooling (`nn.FractionalMaxPool2d`)\n",
    "\n",
    "Similar to max pooling, but allows pooling regions to overlap and vary slightly in size, creating a fractional downsampling effect. Useful in situations where standard integer stride pooling might be too coarse or fine, providing a middle ground for pooling operations.\n",
    "\n",
    "- **Parameters**:\n",
    "  - **`kernel_size`**: The size of the pooling region.\n",
    "  - **`output_size`** or **`output_ratio`**: Specifies the desired output size or ratio, allowing more flexible control over downsampling.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
