{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Q5:** Vanishing Gradient Issue\n",
    "\n",
    "Run the provided network code and observe the gradient norms for each layer. \n",
    "\n",
    "Describe the pattern you see in the gradient values and identify where the vanishing gradient problem is evident.\n",
    "\n",
    "-  Briefly explain why sigmoid activation functions can lead to vanishing gradients, particularly in deep networks.\n",
    "\n",
    "-  Modify the network to use ReLU or Leaky ReLU activations instead of sigmoid. What changes do you see in the gradient norms across layers?\n",
    "\n",
    "-  Experiment with different weight initialization strategies, such as Xavier or He initialization. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Norms Calculation\n",
    "\n",
    "The following code snippet calculates the gradient norms for the parameters (weights) of a neural network model after backpropagation.\n",
    "\n",
    "It helps us in understanding the behavior of the learning process, especially in diagnosing problems such as vanishing gradients.\n",
    "\n",
    "#### Gradient Norm\n",
    "\n",
    "The gradient norm is the L2 norm (Euclidean norm) of the gradient vector for a parameter in the neural network. Gradient norm is defined as:\n",
    "\n",
    "$\n",
    "\\|\\frac{\\partial L}{\\partial \\mathbf{W}^{(l)}}\\|_2 = \\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n} \\left(\\frac{\\partial L}{\\partial w^{[l]}_{i,j}}\\right)^2}\n",
    "$\n",
    "\n",
    "The gradient norm $\\left\\|\\frac{\\partial L}{\\partial \\mathbf{W}^{(l)}}\\right\\|_2$ represents the magnitude of the gradient matrix of the loss function $L$ with respect to the weights $\\mathbf{W}^{(l)}$.\n",
    "\n",
    "Lower gradient norm means smaller gradient values and smaller weight updates. Gradient norm would be zero if gradients with respect to all weight vectors are zero. Therefore, we can use gradient norms to track vanishing gradient issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Hidden Layers\n",
    "L = 50\n",
    "# Number of perceptrons per each hidden layer (assume equal number of perceptrons in each hidden layer for simplicity)\n",
    "k = 1\n",
    "# Number of input features\n",
    "m = 10\n",
    "# Number of output features\n",
    "z = 1\n",
    "# Number of data samples\n",
    "n = 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random input and target\n",
    "# n data samples, each with m features and z target values\n",
    "X = torch.randn(n, m)  \n",
    "Y = torch.randn(n, z)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the network with Sigmoid activations \n",
    "layer_dict = OrderedDict()\n",
    "for i in range(L):\n",
    "    # first hidden layer\n",
    "    if i == 0:\n",
    "        layer_dict[f'lin{i+1}'] = nn.Linear(m, k)\n",
    "    # remaining hidden layers\n",
    "    elif i < (L-1):\n",
    "        layer_dict[f'lin{i+1}'] = nn.Linear(k, k)\n",
    "    else:\n",
    "        # output layer\n",
    "        layer_dict[f'lin{i+1}'] = nn.Linear(k, 1)  # Single perceptron in output layer\n",
    "    layer_dict[f'act{i+1}'] = nn.Sigmoid()\n",
    "\n",
    "# Create the deep neural network model\n",
    "deep_neural_net = nn.Sequential(layer_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- `model.named_parameters()` is a PyTorch method that returns an iterator over the model's parameters, along with their names, allowing inspection or modification of these parameters.\n",
    "   - `name` is the name of the parameter (like 'weight', 'bias') and `param` is the parameter tensor itself.\n",
    "\n",
    "\n",
    "- `if param.grad is not None:` checks whether gradients have been computed for the parameter.\n",
    "   - During backpropagation, gradients may not be computed for some parameters if they are not involved in the computation for the current batch, or if they have been detached from the graph.\n",
    "\n",
    "- `param.grad.norm().item()` computes the norm of the gradient tensor associated with the parameter.\n",
    "   - `param.grad.norm()` calculates the L2 norm (also known as Euclidean norm) of the gradient tensor, which is the square root of the sum of the squares of its elements.\n",
    "   - `.item()` extracts the scalar value from the tensor, making it suitable for appending to a Python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate gradient norms\n",
    "def get_gradient_norms(model, X, Y):\n",
    "    model.zero_grad()\n",
    "    Y_hat = model(X)\n",
    "    loss_func = nn.MSELoss()   #L2 Loss (MSE)\n",
    "    loss = loss_func(Y_hat, Y)  \n",
    "    loss.backward()\n",
    "    gradient_norms = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            gradient_norms.append(param.grad.norm().item())\n",
    "    return gradient_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate gradient norms for the given neural network at each layer\n",
    "gradient_norms_sigmoid = get_gradient_norms(deep_neural_net, X, Y)\n",
    "\n",
    "# Output the gradient norms\n",
    "print(gradient_norms_sigmoid)\n",
    "\n",
    "# Plotting the gradient norms for the vanilla network\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=list(range(1, len(gradient_norms_sigmoid) + 1)),\n",
    "    y=gradient_norms_sigmoid,\n",
    "    mode='lines+markers',\n",
    "    name='Vanilla Sigmoid Network',\n",
    "    marker=dict(size=8),\n",
    "    line=dict(width=2)\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Gradient Norms Across Layers for Vanilla Sigmoid Network',\n",
    "    xaxis_title='Layer Index',\n",
    "    yaxis_title='Gradient Norm',\n",
    "    template='plotly_dark',\n",
    "    hovermode='closest'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
