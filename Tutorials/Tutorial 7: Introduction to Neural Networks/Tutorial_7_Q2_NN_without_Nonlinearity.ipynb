{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Appreciating Activation Functions\n",
        "\n",
        "Here, we show that lack of activation functions in your networks can cause __linear collapse__: a situation where your deep network (with some $L$ layers) collapses to a single layer, being unable to learn complex behaviours in your complex, definitely non-linear data."
      ],
      "metadata": {
        "id": "42tC6eNHse3F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foW2KBJUsaTR"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose we have a 3 layer network. This means we have 3 sets of weights, $W^{[1]}$, $W^{[2]}$, $W^{[3]}$."
      ],
      "metadata": {
        "id": "_UD4DenXslFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W1 = torch.tensor([\n",
        "    [1.4, 0.6],\n",
        "    [0.8, 0.6]\n",
        "])\n",
        "\n",
        "W2 = torch.tensor([\n",
        "    [2.1, -0.5],\n",
        "    [0.7, 1.9]\n",
        "])\n",
        "\n",
        "W3 = torch.tensor([\n",
        "    [1.2, -2.2],\n",
        "    [1.2, 1.3]\n",
        "])\n",
        "\n",
        "x = torch.tensor([\n",
        "    [-1.2],\n",
        "    [1.0]\n",
        "])"
      ],
      "metadata": {
        "id": "vW4pEJH9tEpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WITHOUT SiLU\n",
        "\n",
        "z1 = W1.T @ x\n",
        "z2 = W2.T @ z1\n",
        "out = W3.T @ z2\n",
        "\n",
        "print (out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoRBVGQDuut1",
        "outputId": "a9bdc5ba-5a60-47f5-975a-8135b89a2021"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-2.0640],\n",
            "        [ 4.5260]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we look specifically at the weights, they simply form a chain of matrix products. We can reduce them all to a single $2 \\times 2$ matrix that could have been the initial weight.\n",
        "\n",
        "> This is the same situation as having a single Linear/Dense/Fully-connected layer."
      ],
      "metadata": {
        "id": "RfShlrw6vNgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The original 3-layer network will now perform similarly to\n",
        "a 1-layer network with `W_init`.\n",
        "\n",
        "This can be generalised to any DEEP network with some `n` layers:\n",
        "we can collapse it to a single weight matrix by matrix multiplication.\n",
        "\"\"\"\n",
        "\n",
        "W_init = W3.T @ W2.T @ W1.T\n",
        "print (W_init)\n",
        "print ()\n",
        "\n",
        "out2 = W_init @ x\n",
        "print (out2) # same result as the 3 layer network!!!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UY7MWO2rvgS9",
        "outputId": "8f19d964-1014-4aec-c3fb-9977bdd333db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 4.5600,  3.4080],\n",
            "        [-6.8200, -3.6580]])\n",
            "\n",
            "tensor([[-2.0640],\n",
            "        [ 4.5260]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# WITH SiLU\n",
        "\n",
        "def silu(z):\n",
        "    return z * torch.sigmoid(z)\n",
        "\n",
        "z1 = silu(W1.T @ x)\n",
        "z2 = silu(W2.T @ z1)\n",
        "z2 = W3.T @ z2\n",
        "\n",
        "print (z2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wz8jRb4kuxuc",
        "outputId": "da874a7c-a257-4151-a670-a4447eedb028"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.2369],\n",
            "        [ 0.4730]])\n"
          ]
        }
      ]
    }
  ]
}