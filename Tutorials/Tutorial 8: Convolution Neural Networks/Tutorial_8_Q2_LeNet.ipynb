{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeNet 5 Architecture\n",
    "\n",
    "The LeNet-5 architecture, introduced by Yann LeCun in 1998, is a pioneering convolutional neural network. A variant of LeNet-5 is given below:\n",
    "\n",
    "MNIST Image (28×28) → C1 (28×28×6) → S2 (14×14×6) → C3 (14×14×16) → S4 (7×7×16) → C5 (120) → F6 (84) → Output (10)\n",
    "\n",
    "1. Input: 28x28 grayscale image\n",
    "2. C1: Convolutional layer (6 feature maps, 5x5 kernels)\n",
    "3. S2: Average pooling layer (2x2)\n",
    "4. C3: Convolutional layer (16 feature maps, 5x5 kernels)\n",
    "5. S4: Average pooling layer (2x2)\n",
    "6. C5: Fully connected layer (120 units)\n",
    "7. F6: Fully connected layer (84 units)\n",
    "8. Output: Fully connected layer (10 units)\n",
    "\n",
    "# Do the following\n",
    "\n",
    "- **Q2a** Build LeNet-5 Architecture. Note that the activation function is Sigmoid.\n",
    "\n",
    "- **Q2b** The code provides the parameter count for LeNet-5. Justify the number of parameters through manual calculation of number of parameters.\n",
    "\n",
    "- **Q2c** Modify the code to use ReLU activation instead of Sigmoid. Which activation function is better: ReLU or Sigmoid? Explain.\n",
    "\n",
    "- **Q2d** What changes would you make to this architecture to handle CIFAR-10 images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from random import randint\n",
    "import time\n",
    "# Clear the output and plot updated metrics\n",
    "from IPython.display import clear_output, display\n",
    "import torch\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc, average_precision_score\n",
    "from TutQ2utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    # Check if CUDA is available\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"Using CUDA device.\")\n",
    "\n",
    "    # Check if MPS is available (for Apple M-series chips and newer macOS versions)\n",
    "    elif getattr(torch.backends, 'mps', None) and torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using MPS device.\")\n",
    "    else:\n",
    "        # Fallback to CPU if neither CUDA nor MPS is available\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU.\")\n",
    "    \n",
    "    return device\n",
    "\n",
    "# Get the device\n",
    "device = get_device()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT REMOVE THIS CELL – THIS DOWNLOADS THE MNIST DATASET\n",
    "# RUN THIS CELL BEFORE YOU RUN THE REST OF THE CELLS BELOW\n",
    "#Install torchvision\n",
    "from torchvision import datasets\n",
    "\n",
    "# This downloads the MNIST datasets ~63MB\n",
    "mnist_train = datasets.MNIST(\"./\", train=True, download=True)\n",
    "mnist_test  = datasets.MNIST(\"./\", train=False, download=True)\n",
    "\n",
    "x_train = mnist_train.data \n",
    "y_train = mnist_train.targets\n",
    "    \n",
    "x_test = mnist_test.data \n",
    "y_test = mnist_test.targets \n",
    "\n",
    "# Convert the data to a float type \n",
    "x_train_float = x_train.float()\n",
    "x_test_float = x_test.float()\n",
    "\n",
    "#Compute mean and std\n",
    "#Why?\n",
    "mean = x_train_float.mean()\n",
    "std = x_train_float.std()\n",
    "\n",
    "# Make sure all tensors are on the same device to avoid device mismatch errors\n",
    "mean = mean.to(device)\n",
    "std = std.to(device)\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Q2a** Build LeNet-5 Architecture. Note that the activation function is Sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet_wiki(nn.Module):\n",
    "    def __init__(self):\n",
    "\n",
    "        super(LeNet_wiki, self).__init__()\n",
    "\n",
    "        # Your code goes here\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        #Conv1\n",
    "        x = self.conv1(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        #Your code goes here\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate a LeNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LeNet=LeNet5_convnet()\n",
    "LeNet = LeNet_wiki()\n",
    "LeNet = LeNet.to(device)\n",
    "print(LeNet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Q2b** The code below provides the parameter count for LeNet-5. Justify the number of parameters through manual calculation of number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet5_parameter_count = 0\n",
    "for param in LeNet.parameters():\n",
    "    lenet5_parameter_count += param.numel()\n",
    "print(f'There are {lenet5_parameter_count} parameters in this neural network')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Loss function, learning rate and batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "initial_lr= 0.1 \n",
    "btach_size= 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC curves for untrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, ensure mean and std are on CPU\n",
    "mean = mean.cpu()\n",
    "std = std.cpu()\n",
    "\n",
    "# Now normalize the data\n",
    "x_test_float = x_test.float()\n",
    "x_test_normalized = (x_test_float - mean) / std\n",
    "\n",
    "auc_scores = calculate_roc_curves_test(model=LeNet, x_test=x_test, y_test=y_test, num_classes=10, device=device, mean=mean, std=std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingMetrics:\n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "        self.train_errors = []\n",
    "        self.test_errors = []\n",
    "        self.mean_aucs = []\n",
    "        self.mean_aps = []\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def update(self, train_loss = None, train_error = None, mean_auc=None, mean_ap=None, test_error=None):\n",
    "        self.train_losses.append(train_loss)\n",
    "        self.train_errors.append(train_error)\n",
    "        if mean_auc is not None:\n",
    "            self.mean_aucs.append(mean_auc)\n",
    "        if mean_ap is not None:\n",
    "            self.mean_aps.append(mean_ap)\n",
    "        if test_error is not None:\n",
    "            self.test_errors.append(test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, x_train, y_train, criterion, device, mean, std, batch_size):\n",
    "    \"\"\"\n",
    "    Train for one epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    # running_loss stores the accumulated training loss in an epoch\n",
    "    running_loss = 0\n",
    "    # running_error stores the accumulated training error in an epoch\n",
    "    running_train_error = 0\n",
    "    # number of batches\n",
    "    num_batches = 0\n",
    "    \n",
    "    #batch \n",
    "    shuffled_indices = torch.randperm(len(x_train))\n",
    "    \n",
    "    for count in range(0, len(x_train), batch_size):\n",
    "        #initialize the gradients to zero (to prevent accumulation of gradients)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #select a minibatch\n",
    "        indices = shuffled_indices[count:count + batch_size]\n",
    "        minibatch_data = x_train[indices].unsqueeze(dim=1)\n",
    "        minibatch_label = y_train[indices]\n",
    "        \n",
    "        #Transfer the data to the device \n",
    "        minibatch_data = minibatch_data.float().to(device)\n",
    "        minibatch_label = minibatch_label.to(device)\n",
    "        \n",
    "        #Normalize the input data\n",
    "        #mean and std are the mean and standard deviation of the dataset\n",
    "        inputs = (minibatch_data - mean) / std\n",
    "        inputs = inputs.to(device)\n",
    "        inputs.requires_grad_()\n",
    "        \n",
    "        #scores represent the output of FC layer\n",
    "        # note the absence of SoftMax layer (data is normalized prior to the calculation of loss)        \n",
    "        scores = model(inputs)\n",
    "\n",
    "        #Calculation of loss\n",
    "        loss = criterion(scores, minibatch_label)\n",
    "\n",
    "        #Calculation of gradients with respect to weihghts\n",
    "        loss.backward()\n",
    "\n",
    "        #Updating the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        #Accumulating loss and erorr rate\n",
    "        running_loss += loss.detach().item()\n",
    "        error = error_rate(scores.detach(), minibatch_label)\n",
    "        running_train_error += error.item()\n",
    "\n",
    "        #updating the bacth count\n",
    "        num_batches += 1\n",
    "    \n",
    "    train_loss = running_loss / num_batches\n",
    "    train_error = running_train_error / num_batches\n",
    "    \n",
    "    return train_loss, train_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, x_train, y_train, x_test, y_test, num_epochs, batch_size, initial_lr, device, mean, std):\n",
    "    \"\"\"\n",
    "    Main training loop\n",
    "    \"\"\"\n",
    "    metrics = TrainingMetrics()\n",
    "    criterion = nn.CrossEntropyLoss()    \n",
    "    # Initialize dynamic plots\n",
    "    metrics_plotter = DynamicMetricsPlot()\n",
    "   \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "\n",
    "        # select the optimizer and initialize the learning rate with initial_lr     \n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=initial_lr)\n",
    "        # update the learning rate\n",
    "        current_lr = learning_rate_scheduler(optimizer, epoch, initial_lr)\n",
    "        \n",
    "        # Train for one epoch\n",
    "        train_loss, train_error = train_epoch(model, optimizer, x_train, y_train, criterion, device, mean, std, batch_size)\n",
    "        \n",
    "        # Calculate ROC and PR curves on test set \n",
    "        if epoch % 1 == 0:  \n",
    "            epoch_metrics = calculate_metrics(model, x_test, y_test, num_classes=10,device=device, mean=mean, std=std)\n",
    "            mean_auc = epoch_metrics['mean_auc']\n",
    "            mean_ap = epoch_metrics['mean_ap']\n",
    "            test_error = epoch_metrics['test_error']\n",
    "            #print(test_error)\n",
    "        else:\n",
    "            mean_auc = mean_ap = None\n",
    "\n",
    "        # Update metrics\n",
    "        metrics.update(train_loss = train_loss, train_error = train_error, mean_auc = mean_auc, mean_ap = mean_ap, test_error = test_error)\n",
    "        \n",
    "        # Print statistics\n",
    "        elapsed_time = (time.time() - metrics.start_time) / 60        \n",
    "        print_epoch_stats(epoch, elapsed_time, current_lr, train_loss, train_error, test_error, mean_auc, mean_ap)\n",
    "        \n",
    "        metrics_plotter.update(metrics)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Usage\n",
    "initial_lr = 0.1\n",
    "num_epochs = 15\n",
    "batch_size = 32\n",
    "\n",
    "#metrics = TrainingMetrics()\n",
    "\n",
    "metrics = train_model(\n",
    "    model=LeNet,\n",
    "    x_train=x_train,\n",
    "    y_train=y_train,\n",
    "    x_test=x_test,\n",
    "    y_test=y_test,\n",
    "    num_epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    initial_lr=initial_lr,\n",
    "    device=device,\n",
    "    mean=mean,\n",
    "    std=std\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and plot both ROC and PR curves\n",
    "auc_scores, ap_scores = plot_roc_pr_curves(model=LeNet,  x_test=x_test, y_test=y_test, num_classes=10, device=device,   mean=mean, std=std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a picture at random\n",
    "idx=randint(0, 10000-1)\n",
    "im=x_test[idx]\n",
    "\n",
    "# Display the picture\n",
    "plt.imshow(im.numpy(), cmap='gray')\n",
    "plt.show()\n",
    "#show(im)\n",
    "\n",
    "# send to device, rescale, and view as a batch of 1 \n",
    "im = im.float().to(device)\n",
    "#im= (im-mean) / std\n",
    "im=im.view(1,28,28).unsqueeze(dim=1)\n",
    "\n",
    "# feed it to the net and display the confidence scores\n",
    "scores =  LeNet(im) \n",
    "probs= torch.softmax(scores, dim=1)\n",
    "show_prob_mnist(probs.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Q2c** Modify the code to use ReLU activation instead of Sigmoid. Which activation function is better: ReLU or Sigmoid? Explain.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Q2d** What changes would you make to this architecture to handle CIFAR-10 images?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
