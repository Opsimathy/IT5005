{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip Connection Challenge\n",
    "\n",
    "Skip connections enable very deep neural network through uninterrupted gradient flows. The objective of this question is to study the issues in including the skip connection in a CNN network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1212096f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3a: Implementation of Skip Connection\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider this basic CNN structure shown in the code block below. The objective is to implement the skip connection from the output of Layer 1 to the output of Layer 3. Layer 1's output (a1) should be added to Layer 3's output (a3). The issue with this connection is the shape mismatch between a1 and a3.\n",
    "\n",
    "First answer the following questions. You can assume that the batch size is B and the CIFAR-10 is an RGB dataset with a height and width of 32 pixels each.\n",
    "\n",
    "- **Shape of tensor a1_** : 32x32x16\n",
    "\n",
    "\n",
    "- **Shape of tensor a1**:  16x16x16\n",
    "\n",
    "\n",
    "\n",
    "- **Shape of tensor a2_**: 16x16x32\n",
    "\n",
    "\n",
    "\n",
    "- **Shape of tensor a2**: 8x8x32\n",
    "\n",
    "\n",
    "- **Shape of tensor a3_**: 8x8x64\n",
    "\n",
    "\n",
    "\n",
    "- **Shape of tensor a3**: 4x4x64\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipConnectionCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SkipConnectionCNN, self).__init__()\n",
    "        \n",
    "        # Layer 1: Conv2d(in_channels = 3, out_channels = 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 16, kernel_size=3, stride=1, padding=1)\n",
    "        # MaxPool2d(2, 2)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Layer 2: Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = 3, stride=1, padding=1)\n",
    "\n",
    "        # MaxPool2d(2, 2)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Layer 3: Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3, stride=1, padding=1)\n",
    "                \n",
    "        # Activation function\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Define skip connection from the output of Layer 1 (a1) to output of Layer 3 (a3)\n",
    "        # Hint: a1 and a3 needs to be added and the shapes of a1 and a3 must be compatible for addition\n",
    "        self.SkipConv = nn.Conv2d(in_channels = 16, out_channels = 64, kernel_size = 1)  # 1x1 conv to match channel dimensions\n",
    "        self.SkipPool = nn.MaxPool2d(4,4)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Main path\n",
    "        #Input to Layer 1\n",
    "        a1_ = self.relu(self.conv1(x))     \n",
    "        a1 = self.pool1(a1_)               \n",
    "        #Layer 1 to Layer 2\n",
    "        a2_ = self.relu(self.conv2(a1))    \n",
    "        a2 = self.pool2(a2_)               \n",
    "        #Layer 2 to Layer 3\n",
    "        a3_ = self.relu(self.conv3(a2))    \n",
    "        a3 = self.pool2(a3_)               \n",
    "       \n",
    "        # Implement skip connection from Layer 1 to Layer 3  \n",
    "        # Combine a3 with a1\n",
    "        # a3skip = a3 + a1\n",
    "        # Hint: Check the shapes a1 and a3; they must be compatible for addition\n",
    "        #Matching spatial dimensions (Height and Width)\n",
    "        a1_skip_ = self.SkipConv(a1)\n",
    "        a1_skip = self.SkipPool(a1_skip_)\n",
    "        \n",
    "        a3skip = a3 + a1_skip\n",
    "\n",
    "        return a3skip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3b: Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 3, 32, 32])\n",
      "Output shape: torch.Size([1, 64, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "skip_model = SkipConnectionCNN()\n",
    "\n",
    "# Test the implementation\n",
    "def test_network(model):\n",
    "    # Create random input tensor with CIFAR-10 dimensions\n",
    "    x = torch.randn(1, 3, 32, 32)\n",
    "    \n",
    "\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    \n",
    "    # Print shapes\n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "# Run test\n",
    "test_network(skip_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
