{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colab\n",
    "# Upload the folder containing this file to google drive.\n",
    "import sys, os\n",
    "# Checking if the notebook is opened in google colab\n",
    "#If YES, mount the google drive and change the directory\n",
    "if 'google.colab' in sys.modules:\n",
    "\n",
    "    # mount google drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # change path to the folder\n",
    "    path = 'xxxxx/xxxxx'\n",
    "    print(path)\n",
    "    #os.chdir changes the current working directory\n",
    "    os.chdir(path)\n",
    "    !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from transformer_ import *\n",
    "from utils_gpt_ import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTTextGenerator:\n",
    "    def __init__(self, model_path=None, vocab_filename=None, device=None):\n",
    "        \"\"\"\n",
    "        Initialize generator (for generating indices) along with vocabulary (for mapping indices to words)\n",
    "\n",
    "        Args:\n",
    "            model_path (str): Path to trained model checkpoint\n",
    "            vocab_filename (str): Name of vocabulary text file in same directory\n",
    "            device (str or torch.device or None): Device for generation; if None, auto-detect\n",
    "        \"\"\"\n",
    "\n",
    "        self.device = torch.device(device) if device is not None else get_device()\n",
    "        print(f\"Text Generator initialized on {self.device}\")\n",
    "\n",
    "        # Load model checkpoint data (config + trained weights)\n",
    "        model_data = self._load_model_data(model_path)\n",
    "        # Extract model architecture configuration\n",
    "        self.config = model_data['config']\n",
    "\n",
    "        # Create model architecture with random weights based on saved config\n",
    "        print('Creating GPT with random weights')\n",
    "        self.model = self._create_model(self.config)\n",
    "\n",
    "        # Overwrite random weights with trained parameters from checkpoint\n",
    "        print('Loading the pretrained weights')\n",
    "        self.model.load_state_dict(model_data['state_dict'])\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        print(f\"Model loaded from: {model_path}\")\n",
    "        print(f\"Model parameters: {sum(p.numel() for p in self.model.parameters()):,}\")\n",
    "\n",
    "        # Validate vocabulary filename is provided\n",
    "        if vocab_filename is None:\n",
    "            raise ValueError(\"vocab_filename must be provided\")\n",
    "\n",
    "        # Load vocabulary mappings from text file\n",
    "        self._load_vocabulary_from_text(vocab_filename)\n",
    "        print(f\"Vocabulary loaded from: {vocab_filename}\")\n",
    "        print(f\"Vocabulary size: {len(self.word2idx)}\")\n",
    "\n",
    "        # Sanity check: vocab size must match checkpoint config\n",
    "        if len(self.word2idx) != self.config['vocab_size']:\n",
    "            raise ValueError(\n",
    "                f\"Vocab size mismatch: vocab.txt={len(self.word2idx)} \"\n",
    "                f\"vs model config={self.config['vocab_size']}\"\n",
    "            )\n",
    "\n",
    "    def _load_model_data(self, checkpoint_path):\n",
    "        \"\"\"Load model configuration and weights from checkpoint.\"\"\"\n",
    "        if checkpoint_path is None or not os.path.exists(checkpoint_path):\n",
    "            raise FileNotFoundError(f\"Model checkpoint not found: {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
    "        return {\n",
    "            'config': checkpoint['model_config'],      # Model architecture parameters\n",
    "            'state_dict': checkpoint['model_state_dict']  # Trained model weights\n",
    "        }\n",
    "\n",
    "    def _create_model(self, config):\n",
    "        \"\"\"Create model architecture.\"\"\"\n",
    "        return GPT(**config)\n",
    "\n",
    "    def _load_vocabulary_from_text(self, vocab_filename):\n",
    "        \"\"\"Load vocabulary from text file in same directory.\"\"\"\n",
    "        if not os.path.exists(vocab_filename):\n",
    "            raise FileNotFoundError(f\"Vocabulary file not found: {vocab_filename}\")\n",
    "\n",
    "        with open(vocab_filename, 'r', encoding='utf-8') as f:\n",
    "            words = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "        self.word2idx = {word: idx for idx, word in enumerate(words)}\n",
    "        self.idx2word = words\n",
    "\n",
    "        self.special_tokens = {}\n",
    "        if '<pad>' in self.word2idx:\n",
    "            self.special_tokens['pad_id'] = self.word2idx['<pad>']\n",
    "        if '<unk>' in self.word2idx:\n",
    "            self.special_tokens['unk_id'] = self.word2idx['<unk>']\n",
    "        if '<bos>' in self.word2idx:\n",
    "            self.special_tokens['bos_id'] = self.word2idx['<bos>']\n",
    "        if '<eos>' in self.word2idx:\n",
    "            self.special_tokens['eos_id'] = self.word2idx['<eos>']\n",
    "        if '<mask>' in self.word2idx:\n",
    "            self.special_tokens['mask_id'] = self.word2idx['<mask>']\n",
    "\n",
    "    def text_to_tokens(self, text):\n",
    "        \"\"\"Convert text to token IDs.\"\"\"\n",
    "        # Your data contains only lower case; keep lowercasing to match training\n",
    "        if isinstance(text, str):\n",
    "            words = text.lower().strip().split()\n",
    "        else:\n",
    "            words = text\n",
    "\n",
    "        unk_fallback = self.special_tokens.get('unk_id', self.special_tokens.get('pad_id', 0))\n",
    "        return [self.word2idx.get(w, unk_fallback) for w in words]\n",
    "\n",
    "    def tokens_to_text(self, tokens):\n",
    "        \"\"\"Convert token IDs to text.\"\"\"\n",
    "        if isinstance(tokens, torch.Tensor):\n",
    "            tokens = tokens.tolist()\n",
    "\n",
    "        words_out = []\n",
    "        for token in tokens:\n",
    "            if 0 <= token < len(self.idx2word):\n",
    "                w = self.idx2word[token]\n",
    "                # Show [UNK] for unknowns; skip other specials\n",
    "                if w == '<unk>':\n",
    "                    words_out.append('[UNK]')\n",
    "                elif w not in ['<pad>', '<bos>', '<eos>', '<mask>']:\n",
    "                    words_out.append(w)\n",
    "            else:\n",
    "                words_out.append(f'<UNK_{token}>')\n",
    "        return ' '.join(words_out)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_text(self, prompt_text, max_new_tokens=50, temperature=1.0,\n",
    "                      top_k=None, top_p=None, do_sample=True, verbose=False):\n",
    "        \"\"\"\n",
    "        Generate text from text prompt.\n",
    "        \"\"\"\n",
    "        prompt_tokens = self.text_to_tokens(prompt_text)\n",
    "        if verbose:\n",
    "            print(f\"Input text: '{prompt_text}'\")\n",
    "            print(f\"Input tokens: {prompt_tokens}\")\n",
    "\n",
    "        input_ids = torch.tensor(prompt_tokens, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "        generated_ids = self._generate_tokens(\n",
    "            input_ids, max_new_tokens, temperature, top_k, top_p, do_sample, verbose\n",
    "        )\n",
    "\n",
    "        generated_text = self.tokens_to_text(generated_ids.squeeze())\n",
    "        if verbose:\n",
    "            print(f\"Generated tokens: {generated_ids.squeeze().tolist()}\")\n",
    "            print(f\"Generated text: '{generated_text}'\")\n",
    "        return generated_text\n",
    "\n",
    "    def _generate_tokens(self, input_ids, max_new_tokens, temperature, top_k, top_p, do_sample, verbose):\n",
    "        input_ids = input_ids.to(self.device)\n",
    "        eos_id = self.special_tokens.get('eos_id', None)\n",
    "        max_len = self.config.get('max_seq_length', 1024)\n",
    "\n",
    "        # Truncate prompt if it exceeds max length (keep most recent context)\n",
    "        if input_ids.size(1) > max_len:\n",
    "            input_ids = input_ids[:, -max_len:]\n",
    "\n",
    "        # Validate sampling parameters\n",
    "        if top_k is not None and top_k <= 0:\n",
    "            top_k = None\n",
    "        if top_p is not None:\n",
    "            top_p = float(top_p)\n",
    "            if not (0.0 < top_p <= 1.0):\n",
    "                raise ValueError(\"top_p must be in (0, 1].\")\n",
    "\n",
    "        for step in range(max_new_tokens):\n",
    "            if input_ids.size(1) >= max_len:\n",
    "                if verbose: print(f\"Reached max sequence length at step {step}\")\n",
    "                break\n",
    "\n",
    "            logits, _ = self.model(input_ids)\n",
    "            # model returns logits with shape (B, T, V), where B is batch size, T is sequence length, and V is vocabulary size,\n",
    "            # last element in the sequence correspond to next token\n",
    "            next_token_logits = logits[:, -1, :] / max(temperature, 1e-8)\n",
    "\n",
    "            if top_k is not None:\n",
    "                next_token_logits = self._apply_top_k(next_token_logits, top_k)\n",
    "            if top_p is not None:\n",
    "                next_token_logits = self._apply_top_p(next_token_logits, top_p)\n",
    "\n",
    "            if do_sample and temperature > 0:\n",
    "                probs = F.softmax(next_token_logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "\n",
    "            # Batch=1 assumption: stop when EOS is generated\n",
    "            if eos_id is not None and (next_token == eos_id).all():\n",
    "                if verbose: print(f\"EOS generated at step {step}\")\n",
    "                break\n",
    "\n",
    "        return input_ids\n",
    "\n",
    "    def _apply_top_k(self, logits, top_k):\n",
    "        \"\"\"Apply top-k filtering to limit vocabulary to k most likely tokens.\"\"\"\n",
    "        top_k = min(int(top_k), logits.size(-1))\n",
    "        values, _ = torch.topk(logits, top_k)\n",
    "        min_values = values[:, -1, None]\n",
    "        return torch.where(logits < min_values, torch.full_like(logits, -float('inf')), logits)\n",
    "\n",
    "    def _apply_top_p(self, logits, top_p):\n",
    "        \"\"\"Apply nucleus sampling: keep tokens until cumulative probability exceeds top_p.\"\"\"\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        sorted_probs = F.softmax(sorted_logits, dim=-1)\n",
    "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "\n",
    "        # Mask tokens to remove\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Ensure at least one token remains\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        # Scatter back to original indices\n",
    "        indices_to_remove = torch.zeros_like(logits, dtype=torch.bool)\n",
    "        indices_to_remove.scatter_(1, sorted_indices, sorted_indices_to_remove)\n",
    "\n",
    "        return logits.masked_fill(indices_to_remove, -float('inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_generation_comparison(generator, prompts, temperatures, max_new_tokens=30):\n",
    "    # Define strategies \n",
    "    strategies = [\n",
    "        (\"Greedy\",        {\"do_sample\": False}),\n",
    "        (\"Top-k (None)\",  {\"do_sample\": True}),                   # sampling with temperature only\n",
    "        (\"Top-k k=40\",    {\"do_sample\": True, \"top_k\": 40}),\n",
    "        (\"Top-k k=100\",   {\"do_sample\": True, \"top_k\": 100}),\n",
    "        (\"Top-p (None)\",  {\"do_sample\": True}),                   # sampling with temperature only\n",
    "        (\"Top-p p=0.9\",   {\"do_sample\": True, \"top_p\": 0.9}),\n",
    "        (\"Top-p p=0.95\",  {\"do_sample\": True, \"top_p\": 0.95}),\n",
    "    ]\n",
    "\n",
    "    for prompt in prompts:\n",
    "        print(\"Prompt:\")\n",
    "        print(prompt)       \n",
    "\n",
    "        for temp in temperatures:\n",
    "            print(f\"\\n--- Temperature: {temp:.2f} ---\\n\")\n",
    "            for name, params in strategies:\n",
    "                try:\n",
    "                    text = generator.generate_text(\n",
    "                        prompt_text=prompt,\n",
    "                        max_new_tokens=max_new_tokens,\n",
    "                        temperature=temp,\n",
    "                        top_k=params.get(\"top_k\", None),\n",
    "                        top_p=params.get(\"top_p\", None),\n",
    "                        do_sample=params.get(\"do_sample\", True),\n",
    "                        verbose=False\n",
    "                    )\n",
    "                    print(f\"[{name}]\")\n",
    "                    print(text.strip() + \"\\n\")\n",
    "                except Exception as e:\n",
    "                    print(f\"[{name}]\")\n",
    "                    print(f\"Error: {e}\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts and temperatures (matching your example)\n",
    "prompts = [\n",
    "        \"fed officials signaled that\",\n",
    "        # add more prompts if desired\n",
    "    ]\n",
    "\n",
    "temperatures = [0.70, 0.90, 1.10]\n",
    "\n",
    "generator = GPTTextGenerator(model_path='trained_gpt_model.pt', vocab_filename='vocabulary.txt', device=None)\n",
    "\n",
    "print_generation_comparison(generator, prompts, temperatures, max_new_tokens=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "it5005",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
